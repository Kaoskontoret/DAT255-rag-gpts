{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !IMPORTANT!\n",
    "## AI have been used to develop parts of this notebook. This includes github copilot and llm services for solving complex problems. It is important for us to not give an impression that we have scraped this out of our own mind. We are however responsible for selecting methods and making knowledgebased decitions to reduce trainingtime, imporve performance, and select proper tools for the job. This project has the goal of learning more about how to train and evaluate BERT models with data of our choice, using Torch modules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frist we need to load our dataset\n",
    "We have selected the NbAiLab/NCC dataset\n",
    "This is a dataset contructed by the national library optimized for training large language models\n",
    "We only want the nynorsk eksamples since our bert model are focusing on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "\n",
    "dataset = load_dataset(\"NbAiLab/NCC\", streaming=True)\n",
    "\n",
    "nynorsk_dataset = dataset[\"train\"].filter(lambda example: example.get(\"lang_fasttext\", \"\") == \"nn\")\n",
    "\n",
    "with open(\"nynorsk_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, example in enumerate(islice(nynorsk_dataset, 500)):\n",
    "        f.write(example[\"text\"].strip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'maalfrid_4b84113933f7134b94f68fbf804a70405b08e68e_23', 'doc_type': 'maalfrid_ssb', 'publish_year': 2021, 'lang_fasttext': 'nn', 'lang_fasttext_conf': '0.446', 'text': 'u) Nr ul c) u) o_ Nr Nr 001 0) 0 1,- 0) e-- CiD CO CD CD 0 Nt LO Nr Tr Co Tr ,- Nr c) c4 0) co_ 0) co r- Tr c» o Nr ul u) Tr co r- Nr a) Nr u)'}\n",
      "{'id': 'maalfrid_f1ebddd1220542416ef98935770372f0fc8c011b_61', 'doc_type': 'maalfrid_regjeringen', 'publish_year': 2021, 'lang_fasttext': 'nn', 'lang_fasttext_conf': '0.607', 'text': 'Boktittelen Språk er makt av psykologen Rolv Mik\\xad kel Blakar frå 1973 opna eit nytt perspektiv på språk og vart raskt eit munnhell. No er det også vorte vanleg å seia at språk er ei viktig kjelde til makt, eller at språk og språkbruk representerer makt. Språk og språkbruk er integrert i samfunnet, «der kontroll, påvirkning og makt er uttrykk for sentrale mellommenneskelege relasjonar», skriv Blakar. Men språk er ikkje alltid makt, og makt dreier seg om mykje meir enn språk og språkbruk. I det daglege gjev språk makt ved at nokon defi\\xad nerer situasjonar, markerer tilknyting eller avstand, gruppefellesskap eller sosialt overtak. Dette kan gjerast ut frå ein målretta og medviten strategi, eller utan at den som utøver makta, ten\\xad kjer over kva som skjer. Språkmakt kan koma frå språket sjølv, frå det å ha ordet i si makt, eller frå ytre omstende. Den makta som ligg i sjølve språket, har å gjera med både form og innhald i det som blir sagt eller skrive. Den makta som ligg utanfor språket, kjem sær\\xad leg frå den som ytrar seg, den gruppa vedkomande høyrer til, eller den status som språksystemet har. Mest språkmakt har gjerne den som både meistrar det språklege uttrykket og har ein sosial posisjon som gjev ekstra respekt eller truverde. Det som er tema i denne meldinga, er språk og språkbruk i det offentlege rommet, utanfor privat\\xad livet og familiens fire vegger. Vi ser bort frå den typen uformell språkleg makt som ligg i sjølve språksystemet og i det å meistra språklege verke\\xad middel, med mindre slik makt har eller kan få føl\\xad gjer for samfunnsutforminga og politiske avgjer\\xad der. Den makta som ligg i språket, kjem her i bak\\xad grunnen for den språklege makta som ligg i det å kunna bruka språket og delta i dei demokratiske prosessane i samfunnet. Drøftinga i dette kapitlet er ordna etter ulike sider i tilhøvet mellom makt og språk. Forholdet mellom dei to offisielle målformene er både formelt og språkleg annleis enn forholdet mellom norsk og samisk, norsk og minoritetsspråk, norsk og engelsk. Men ved at vi her i prinsippet drøftar makt på tvers av slike språkskilje, kan både felles\\xad trekk og skilnader bli tydelegare. Dette kan gje ei 3 Blakar, Rolv Mikkel 2006: 225. meir samla forståing av forholdet mellom makt, språk og demokrati i Noreg. I Noreg er det gjennomført to store maktutgreiin\\xad gar. Den første vart gjennomført under leiing av Gudmund Hernes i åra 1972–82. Den andre, under leiing av Øyvind Østerud i perioden 1998–2003, er også nemnd ovanfor. Den første utgreiinga skulle «skaffe til veie et best mulig kjennskap til de reelle maktforhold i det norske samfunn». Ut frå ei forståing av «maktut\\xad øvelse som viljesutøvelse» var utgreiinga konsent\\xad rert om fordeling av økonomisk og politisk makt, og om forholdet mellom statsmakt og marknads\\xad makt. I sluttrapporten vart den makt massemedia har, drøfta i eit lengre kapittel, men ikkje forholdet mel\\xad lom makt og språk. Hernes skreiv i det teoretiske grunnlagsdokumentet Makt og avmakt eit avsnitt som dels bygde på framstillinga til Blakar. Der hei\\xad ter det: «Språket har makt over tanken fordi det gjør det vanskeligere å tenke i andre baner. Språket kan også brukes til å utelukke andre fra forstå\\xad else, og dermed fra meningsberettigelse og deltakelse. Det språk som er knyttet til flere profesjoner tar til dels sikte på at diskusjonen mellom deres utøvere skal gå over hodene på folk slik at de ikke blander seg inn.» Ei deloppgåve i mandatet var å «klarlegge grup\\xad per som har liten innflytelse over sin egen situa\\xad sjon», men dette vart i liten grad følgt opp i dei mange bøkene og skriftene frå denne første norske maktutgreiinga. Perspektiva på makt vart utvida i 1980- og 90\\xad åra, ikkje minst ved at søkjelyset vart retta mot kva rolle kommunikasjon og språk har i samfunnet. Dette viser att i den andre maktutgreiinga. Hovud\\xad temaet i dette utgreiingsarbeidet var «vilkårene for det norske folkestyret og endringer i disse». Her skulle ein mellom anna vurdera kva sosiale, økono\\xad miske og kulturelle skiljelinjer, alder og kjønn kunne ha å seia for høve til å delta og påverka. Utgreiinga skulle på den eine sida avgrensa merk\\xad semda til forholdet mellom makt og demokrati, og på den andre sida drøfta dette frå så mange syns\\xad vinklar at den samla drøftinga ville bli ganske mykje vidare enn i den første maktutgreiinga. Sluttrapporten er NOU 2003:19 Makt og demo\\xad krati. I tillegg kom det 76 skrifter i ein rapportserie, 51 boktitlar og store mengder fagartiklar og kro\\xad 4 Petersson, Olof 1987:'}\n",
      "{'id': 'maalfrid_fcc34148f00916730145e534fe2e41f97089193d_12', 'doc_type': 'maalfrid_nlr', 'publish_year': 2021, 'lang_fasttext': 'nn', 'lang_fasttext_conf': '0.461', 'text': 'Skjellanger, Meland Beiteområdet til Skjellanger beitelag ligg nord i Meland kommune, mellom utmarksgrensa til Husebø i søraust og Skjellanger fort i nordvest. Arealet er kystlynghei med innslag av myr, furuskog, einer og plantefelt med gran. Folk som bur her, nyttar området til å gå turar. To av bøndene har gått saman om skjøtsel og beiting av arealet, dei har stifta beitelag og utarbeidd beitebruksplan. Tal bruk 7 Tal bønder 2 Dyreslag (tal) Storfe (7), sau (20), nokon geit Areal 900 da Spesielle verdiar Kystlynghei, friluftsområde Tilskot SMIL (2005): 90 000 kr RMP (2005): 10 000 kr Tiltak i beitebruksplanen:'}\n",
      "{'id': 'bruvik_null_null_19710430_20_16_1_MODSMD_ARTICLE6', 'doc_type': 'newspaper_ocr', 'publish_year': 1971, 'lang_fasttext': 'nn', 'lang_fasttext_conf': '0.677', 'text': 'hadde årsmøte i Bergen med helga. Det vart bore fram helsingar frå Folkeakademienes Landsfor bund ved Arna Folkedal og frå Hordaland Fylkesopplysinngsråd ved Johs. Grasdal. Kontaktutvalet for populær vitskapleg verksemd ved Univer sitetet i Bergen var representert ved formannen, dosent Egil Bak ka, som heldt foredrag om «Kris tendom i vikingebygdene i vest». og gjorde greie for tilbod om føre lesarar og kursemne frå Univer sitetet. Av årsmeldinga gjekk det fram at aktiviteten hadde vore større enn nokon gong før. 26 akademi hadde gjeve melding om 370 til skipingar med tils. 32 442 fram motte (tala frå året for var: 322 tilskipingar og 29 172 frammøtte). Av dette var det 8 turnear med 97 tilskipingar. Det største arrangementet var Vigelandsutstillinga, som vart vitja av vel 2500 personar. I høve Naturvernåret 1970 kan | nemnast emna: «Kjemi, biologi og naturvern», «Naturvern, helse og politikk», «Mennesket i natur en», «Biologi og menneskeverd», i«Kjeldevatn kloakk», «Indu striområde bustadfelt». Litterære emne var: «Glytt inn i diktarverkstaden», «Arets litteratur», «Dans ropte fela», ! «Vagantviser», «Julesongar, jule ! opptøy», «Biblioteket og studie ! arbeidet». «Vaksenopplæring» var I også eit aktuelt emne. drag om «Ungdom og medika mentmisbruk-», «Narkotikapro blemet», «Abortproblemet» og «Hvordan kan vi bygge opp bar nekroppen». Andre emne var: «Korleis levde fedre våre?» og «FN idag». Desse musikkprogram vart gjevne : Ingebjørg Gresvik —S. Torstenson: Edvard Grieg, Lasse Kolstad Tor Hultin: Viser, Liv Dommersnes Liv Glaser: Ly rikk musikk, Ruth Reese: Neg ro Spirituals. Elles er det i stor mon nytta lokale krefter. Det kan trygt seiast at mange akademi har gjeve eit rikt og mangesidig tilbod. Program med tilknyting til det lokale bygde samfunnet har samla mykje folk. Ei rekkje akademi står som til skiparar på det lokale plan for Riksgalleriet og Riksteatret. Den mangeårige leiaren av akademiverksemda i Hordaland, Svein Simonsen, vart varmt tak ka og hylla av årsmøtet. Styret for Hordaland Folkeaka demi er: Arna Folkedal, Ulvik, (varaform.) Johs. Grasdal, Stord. Svein Simonsen, Arna og K. N Hjeltnes, Kvam (form.)'}\n",
      "{'id': 'firdafolkeblad_null_null_19400423_35_32_1_MODSMD_ARTICLE8', 'doc_type': 'newspaper_ocr', 'publish_year': 1940, 'lang_fasttext': 'nn', 'lang_fasttext_conf': '0.563', 'text': 'Kampane sundag. Hærens overkommando mel der den 22. april : Den 21. april heldt tyskarane fram med sine åtak på stillingane våre på båe sidor av Mjøsa stotta av artilleri, stridsvogner Øog fly. Tyskarane gjekk Øog på stillingane våre i Øysterdalen. I Valdres gjekk våre tropper fram Øog tok 200 fangar. Tyske fly bomoa norske sam bandslinor på fleire stader utan nemnande brot i trafikken. Tys karane kasta Øog ned flygeblad. 5 tyske fly vart nenskotne. På tyske fangar finn ein patroner der pro_ jektiler verkar som dum-dum.'}\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"NbAiLab/NCC\", split=\"validation\", streaming=True)\n",
    "\n",
    "test_nynorsk_dataset = test_dataset.filter(lambda example: example.get(\"lang_fasttext\", \"\") == \"nn\")\n",
    "\n",
    "for example in islice(test_nynorsk_dataset, 5):\n",
    "    print(example)\n",
    "\n",
    "with open(\"nynorsk_test_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, example in enumerate(islice(test_nynorsk_dataset, 800)):\n",
    "        f.write(example[\"text\"].strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and settin special tokens\n",
    "Before we start creating hour BERT model we first define our tokenizer. The tokenizer builds a tokenizermodel working with a set vocabulary size.\n",
    "\n",
    "It is also to define some special tokens. These tokens each have different responsibilities:\n",
    "\n",
    "-  [PAD] tokens are added to sentences so all of them have equal length\n",
    "-  [CLS] tokens are Classification tokens, it serves as a Start of sentence(SOS) and represent the meaning of the sentence\n",
    "- [SEP] represents the End of entence(EOS) and also separation between sentences\n",
    "- [MASK] is used for wordreplacement during masked language tasks\n",
    "- [UNK] is used for filling in for words that are out of the vocabulary og the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "tokenizer.train(\n",
    "    files=[\"nynorsk_corpus.txt\"],\n",
    "    vocab_size=10000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
    ")\n",
    "\n",
    "tokenizer.save_model(\"tokenizer/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just a small script for loading in the tokenizer model from a directory\n",
    "Now we do not need to train it every time we want to test something quickly. Yippieee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Example tokens: ['dette', 'er', 'ein', 'test', 'for', 'nynorsk', '.']\n",
      "Vocabulary size: 10000\n",
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "# load_tokenizer.py\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "print(\"Tokenizer loaded. Example tokens:\", tokenizer.tokenize(\"Dette er ein test for Nynorsk.\"))\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCCTorchDataset\n",
    "Before sending our dataset into a BERT model we need to prepare it.\n",
    "In this class we use the tokenizer we trained earlier to encode the text included in the dataset.\n",
    "\n",
    "Here an important feature to mention is the DataCollectorForLanguageModdeling class. This masks a procentage of the dataset so it will be prime to use for training a BERT model for mlm tasks. For each batch that are pulled from the dataloader a new masking pattern is created, securing that there are differences in each batch. In this cell we also create a dev set and a train set. The trainset is used to train the model later. And the dev set is used for evaluating the different models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating alle split: 800 examples [00:00, 82094.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch shapes:\n",
      "  input_ids      : (32, 128)\n",
      "  attention_mask : (32, 128)\n",
      "  token_type_ids : (32, 128)\n",
      "  labels         : (32, 128)\n",
      "\n",
      "Dev batch shapes:\n",
      "  input_ids      : (32, 128)\n",
      "  attention_mask : (32, 128)\n",
      "  token_type_ids : (32, 128)\n",
      "  labels         : (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "class NCCTorchDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer: BertTokenizer, max_len=128):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.max_len    = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.hf_dataset[index]['text']\n",
    "        enc  = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids      = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        token_type_ids = torch.zeros_like(input_ids, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "hf_dataset = load_dataset(\"text\", data_files={\"alle\": \"nynorsk_corpus.txt\"}, split=\"alle\")\n",
    "split = hf_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_hf = split['train']\n",
    "dev_hf   = split['test']\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "\n",
    "train_ds = NCCTorchDataset(train_hf, tokenizer, max_len=128)\n",
    "\n",
    "dev_ds = NCCTorchDataset(dev_hf, tokenizer, max_len=128)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Train batch shapes:\")\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    print(f\"  {k:15s}: {tuple(v.shape)}\")\n",
    "\n",
    "print(\"\\nDev batch shapes:\")\n",
    "batch = next(iter(dev_loader))\n",
    "for k, v in batch.items():\n",
    "    print(f\"  {k:15s}: {tuple(v.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the good stuff\n",
    "In the following cell we have prepared classes for positional embedding, BERT embedding and a CustomBERT model that we are going to train later. \n",
    "\n",
    "## Postitional embedding\n",
    "\n",
    "Positional embedding is the first part of the pipeline. It is a clever way of to destingush where in a sequence a word is placed, and also the distance to other words in the sequence. Sine and cosine functions are used to create unique positional encodings for each position in the sequence\n",
    "\n",
    "## BERT embedding\n",
    "The BERTembedding class is used to create input embeddings for the BERT model\n",
    "It combines token embeddings, segment embeddings and positional embeddings.\n",
    "The token embeddings are learned from the vocabularym while the segment embeddings are used tp diffriencate between two segments in the input. The positional embeddings are added to the token embeddings to provide information about the position of each token. Dropout layer is to prevent overfitting...\n",
    "\n",
    "# CustomBERT\n",
    "The CustomBERT class is a simplified version of the BERT model.\n",
    "It consists of an embedding layer, a transformer encoder, and a masked language modeling (MLM) head.\n",
    "The transformer encoder is composed of multiple layers, each containing multi-head self-attention\n",
    "and feedforward neural networks.\n",
    "The MLM head is used to predict the masked tokens in the input sequence.\n",
    "The forward method takes token IDs, segment IDs, and an optional attention mask as input,\n",
    "and returns the logits for the masked language modeling task.\n",
    "The save_model method allows saving the model's state dictionary to a specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.segment_embedding = nn.Embedding(2, d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(d_model, seq_len)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, token_ids, segment_ids):\n",
    "        x = self.token_embedding(token_ids) + self.segment_embedding(segment_ids)\n",
    "        x = self.positional_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, seq_len=128, num_layers=4, num_heads=4, dim_feedforward=512):\n",
    "        super().__init__()\n",
    "        self.embeddings = BERTEmbedding(vocab_size, d_model, seq_len)        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Masked Language Modeling head: project encoder output to the vocabulary space.\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, token_ids, segment_ids, attention_mask=None):\n",
    "        # token_ids, segment_ids: [batch_size, seq_len]\n",
    "        x = self.embeddings(token_ids, segment_ids)  # shape: [batch_size, seq_len, d_model]\n",
    "        # PyTorch's Transformer encoder expects input shape: [seq_len, batch_size, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        # Create key padding mask if provided (mask positions where attention_mask==0)\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None \n",
    "        # Pass through the encoder layers.\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        # Transform back to [batch_size, seq_len, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        # Compute MLM logits.\n",
    "        logits = self.mlm_head(x)\n",
    "        return logits\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we train our own customBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following steps are used\n",
    "\n",
    "Create a model and what hyperparameters it should have.\n",
    "\n",
    "Optimizer\n",
    "We use an adamW optimizer for updating the models parameters based on gradients computed during back propagation\n",
    "\n",
    "Loss function\n",
    "The loss function is a crossentropu loss that computes the difference between our models predicted token distribution, and the \"true\" tokens it want it to predikt\n",
    "by setting all unmasked positions in the labels to -100, we kan ignore index -100 to skip those positions when averaging the loss. This penalizes the model for getting masked tokens wrong, matching BERT mlm objective\n",
    "\n",
    "GradScaler\n",
    "Then we use a gradient scaler. This utilizes 16.bit operations which uses a lott less memory than the 32 bit operations. CHATGPT said this will help so we tried with trainingtimes...\n",
    "\n",
    "## Then the epochs\n",
    "\n",
    "each epoch loops trhough the whole dataset once. The first lines load the tensors from the batch. Then zero_grad() is added to clear gradient buffers from previous steps.\n",
    "Autocast enables mixed-precision, and reshapes the tensors from [B*L,V] whre B are batch size, L sequence lenght and V the Vocab_length, down to [B*L] this makes it possible for CrossEntropyLoss to compute the average negative log-likelygood over the masked tokens\n",
    "\n",
    "Backpropagation is done with the gradscaler to correct the model taking the los as an input\n",
    "\n",
    "These teqniques were not used earlier for some models.\n",
    "\n",
    "## NB\n",
    "We changed this cell in the last minute trying to use both a training set and a validationset calculation in training. We did however not have enough time to finnish the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [train]:   0%|          | 0/23 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=4, num_heads=12, hidden_dim=3072)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# 2) Scheduler with linear warmup & decay\n",
    "num_epochs    = 3\n",
    "total_steps   = num_epochs * len(dataloader)\n",
    "warmup_steps  = int(0.1 * total_steps)\n",
    "scheduler     = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "scaler = torch.GradScaler(\"cpu\")\n",
    "\n",
    "model.train()\n",
    "num_epochs = 3\n",
    "\n",
    "print(\"Training started...\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(dataloader, total=len(dataloader), desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
    "\n",
    "    for step, batch in enumerate(loop, start=1):\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed‐precision forward\n",
    "        with torch.autocast(\"cpu\"):\n",
    "            logits = model(\n",
    "                token_ids     = input_ids,\n",
    "                segment_ids   = token_type_ids,\n",
    "                attention_mask= attention_mask\n",
    "            )\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, VOCAB_SIZE),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "        # Scale, backward, unscale, clip, step\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        avg_loss = running_loss / step\n",
    "        loop.set_postfix(avg_loss=f\"{avg_loss:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f\"*** Epoch {epoch} complete — avg loss: {epoch_loss:.4f} ***\")\n",
    "\n",
    "\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"custom_bert_model/mlm_finetuned2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 predictions for [MASK]: ['.', 'i', \"'\", ',', '##ד', '##el', '##ed', '##in', '##et', '##en']\n",
      "Original sentence: Dette er ein [MASK] for Nynorsk.\n",
      "Masked token ID: 18\n",
      "Masked token: [MASK]\n",
      "Prediction for [MASK]: .\n",
      "Dette er ein . for Nynorsk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer  # pip install transformers\n",
    "\n",
    "# 1) Load tokenizer & model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "# Model trained on 500 examples with a VOCAB_SIZE of 10000, and tuned parameters, + masked data and backpropagation.\n",
    "model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=4, num_heads=12, dim_feedforward==3072)\n",
    "\n",
    "\n",
    "# Load the custom BERT model with tuned parameters trained on 2000 examples.\n",
    "#model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=12, num_heads=12, dim_feedforward=512)\n",
    "\n",
    "\n",
    "# Load model tuned with parameters trained on 8000 examples.\n",
    "# model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "\n",
    "# Load model with smaller parameters trainied on 10000 examples.\n",
    "# model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "\n",
    "\n",
    "# Load model trained on 100000 examples using the same parameters as the small, with a gpu.\n",
    "#model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "\n",
    "# Load model from directory.\n",
    "current_model = \"mlm_finetuned.pth\"\n",
    "checkpoint = torch.load(\n",
    "    \"custom_bert_model/\" + current_model,\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# 2) Prepare a masked sentence\n",
    "sentence = \"Dette er ein [MASK] for Nynorsk.\"\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "token_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "segment_ids = torch.zeros_like(token_ids)\n",
    "\n",
    "# 3) Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(token_ids, segment_ids, attention_mask)\n",
    "\n",
    "# 4) Find masked position(s) and predict\n",
    "mask_token_index = (token_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "batch_idx, seq_pos = mask_token_index[0][0], mask_token_index[1][0]\n",
    "mask_logits = logits[batch_idx, seq_pos]\n",
    "# Get the 5 most likely tokens for the masked position.\n",
    "top_k = 5\n",
    "top_k_indices = torch.topk(mask_logits, top_k).indices\n",
    "top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices.tolist())\n",
    "# Print the top-k predictions\n",
    "print(f\"Top {top_k} predictions for [MASK]: {top_k_tokens}\")\n",
    "# 5) Get the predicted token ID and convert it to a token\n",
    "predicted_id = mask_logits.argmax(dim=-1).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_id)\n",
    "# 5) Print the prediction\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Masked token ID: {predicted_id}\")\n",
    "print(f\"Masked token: {tokenizer.mask_token}\")\n",
    "print(f\"Prediction for [MASK]: {predicted_token}\")\n",
    "print(f\"Dette er ein {predicted_token} for Nynorsk.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This cell evaluates the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBert Evaluation results on dev set:\n",
      "  avg_loss: 11.1442\n",
      "  perplexity: 69164.2232\n",
      "  top1_accuracy: 0.0635\n",
      "  top5_accuracy: 0.0642\n",
      "\n",
      " Custombert2 Evaluation results on dev set:\n",
      "  avg_loss: 10.5152\n",
      "  perplexity: 36871.4029\n",
      "  top1_accuracy: 0.0430\n",
      "  top5_accuracy: 0.0909\n",
      "\n",
      "MLM Finetuned Evaluation results on dev set:\n",
      "  avg_loss: 8.4661\n",
      "  perplexity: 4750.8034\n",
      "  top1_accuracy: 0.0395\n",
      "  top5_accuracy: 0.1118\n",
      "\n",
      "I1 CustomBert Evaluation results on dev set:\n",
      "  avg_loss: 12.4231\n",
      "  perplexity: 248486.2151\n",
      "  top1_accuracy: 0.0536\n",
      "  top5_accuracy: 0.0536\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict\n",
    "\n",
    "def evaluate_mlm(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    vocab_size: int,\n",
    "    device: torch.device\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a masked language model on a dataloader.\n",
    "\n",
    "    Returns a dictionary with:\n",
    "      - avg_loss: float\n",
    "      - perplexity: float\n",
    "      - top1_acc: float\n",
    "      - top5_acc: float\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_masked = 0\n",
    "    correct1 = 0\n",
    "    correct5 = 0\n",
    "    vocab_size = vocab_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids      = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                token_ids = input_ids,\n",
    "                segment_ids = token_type_ids,\n",
    "                attention_mask = attention_mask\n",
    "            )  # [B, L, V]\n",
    "\n",
    "            # Compute loss over masked tokens\n",
    "            loss = loss_fn(logits.view(-1, vocab_size), labels.view(-1))\n",
    "            num_masked = (labels.view(-1) != -100).sum().item()\n",
    "            total_loss += loss.item() * num_masked\n",
    "            total_masked += num_masked\n",
    "\n",
    "            # Compute top-k accuracy\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            mask_pos = labels != -100\n",
    "            masked_preds = probs[mask_pos]  # [N_masked, V]\n",
    "            true_ids = labels[mask_pos]     # [N_masked]\n",
    "\n",
    "            # Top-1\n",
    "            pred1 = masked_preds.argmax(dim=-1)\n",
    "            correct1 += (pred1 == true_ids).sum().item()\n",
    "\n",
    "            # Top-5\n",
    "            top5 = torch.topk(masked_preds, k=5, dim=-1).indices  # [N_masked, 5]\n",
    "            for i in range(top5.size(0)):\n",
    "                if true_ids[i].item() in top5[i].tolist():\n",
    "                    correct5 += 1\n",
    "\n",
    "    avg_loss = total_loss / total_masked\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    top1_acc = correct1 / total_masked\n",
    "    top5_acc = correct5 / total_masked\n",
    "\n",
    "    return {\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"top1_accuracy\": top1_acc,\n",
    "        \"top5_accuracy\": top5_acc\n",
    "    }\n",
    "\n",
    "custom_bert1 = CustomBERT(vocab_size=30000, d_model=256, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "checkpoint = torch.load(\n",
    "    \"custom_bert_model/custom_bert_model.pth\",\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "custom_bert1.load_state_dict(checkpoint)\n",
    "custom_bert1.to(device)\n",
    "custom_bert1.eval()\n",
    "\n",
    "custom_bert2 = CustomBERT(vocab_size=30000, d_model=768, seq_len=128,num_layers=12, num_heads=12, dim_feedforward=512)\n",
    "checkpoint = torch.load(\n",
    "    \"custom_bert_model/custom_bert_model2.pth\",\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "custom_bert2.load_state_dict(checkpoint)\n",
    "custom_bert2.to(device)\n",
    "custom_bert2.eval()\n",
    "\n",
    "# Evaluate the models\n",
    "dev_results = evaluate_mlm(\n",
    "    model=custom_bert1,\n",
    "    dataloader=dev_loader,\n",
    "    vocab_size=30000,\n",
    "    device=device\n",
    ")\n",
    "print(\"CustomBert Evaluation results on dev set:\")\n",
    "for key, value in dev_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "cb_2_results = evaluate_mlm(\n",
    "    model=custom_bert2,\n",
    "    dataloader=dev_loader,\n",
    "    vocab_size=30000,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n Custombert2 Evaluation results on dev set:\")\n",
    "for key, value in cb_2_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "mlmf_results = evaluate_mlm(\n",
    "    model=model,\n",
    "    dataloader=dev_loader,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    device=device\n",
    ")\n",
    "print(\"\\nMLM Finetuned Evaluation results on dev set:\")\n",
    "for key, value in mlmf_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "#model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "# Load model from directory.\n",
    "i1_bert = CustomBERT(vocab_size=30000, d_model=256, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "checkpoint = torch.load(\n",
    "    \"custom_bert_model/i1_custom_bert_model.pth\",\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "i1_bert.load_state_dict(checkpoint)\n",
    "i1_bert.to(device)\n",
    "i1_bert.eval()\n",
    "\n",
    "# Evaluate the models   \n",
    "i1_results = evaluate_mlm(\n",
    "    model=i1_bert,\n",
    "    dataloader=dev_loader,\n",
    "    vocab_size=30000,\n",
    "    device=device\n",
    ")\n",
    "print(\"\\nI1 CustomBert Evaluation results on dev set:\")\n",
    "for key, value in i1_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
