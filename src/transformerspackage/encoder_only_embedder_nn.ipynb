{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mNbAiLab/NCC\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m nynorsk_dataset = dataset.filter(\u001b[38;5;28;01mlambda\u001b[39;00m example: example.get(\u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mnn\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m dataloader = \u001b[43mtf\u001b[49m.data.Dataset.from_generator(\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: (example[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m nynorsk_dataset),\n\u001b[32m      7\u001b[39m     output_signature=tf.TensorSpec(shape=(), dtype=tf.string)\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(nynorsk_dataset)\n",
      "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"NbAiLab/NCC\")\n",
    "\n",
    "nynorsk_dataset = dataset.filter(lambda example: example.get(\"language\", \"\") == \"nn\")\n",
    "\n",
    "\n",
    "print(nynorsk_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize_and_train.py\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Initialize a new tokenizer instance with appropriate options.\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "\n",
    "# Train the tokenizer:\n",
    "tokenizer.train(\n",
    "    files=nynorsk_dataset[\"train\"][\"text\"],\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
    ")\n",
    "\n",
    "# Save the tokenizer to a directory for later use.\n",
    "tokenizer.save_model(\"tokenizer/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Example tokens: ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "# load_tokenizer.py\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the custom tokenizer from the saved directory.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "print(\"Tokenizer loaded. Example tokens:\", tokenizer.tokenize(\"Dette er en test for Nynorsk.\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sample: {'input_ids': tensor([1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# dataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class NCCDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer: BertTokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        # Encode the text using the tokenizer and include special tokens.\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(),         # shape: [max_len]\n",
    "            'attention_mask': encoded['attention_mask'].squeeze()    # shape: [max_len]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits shape: torch.Size([8, 128, 30522])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# =======================\n",
    "# 1. Custom Embedding Modules\n",
    "# =======================\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # shape: [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, d_model]\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # Here we assume 2 segments (e.g. sentence A and sentence B)\n",
    "        self.segment_embedding = nn.Embedding(2, d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(d_model, seq_len)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, token_ids, segment_ids):\n",
    "        # token_ids and segment_ids: [batch_size, seq_len]\n",
    "        x = self.token_embedding(token_ids) + self.segment_embedding(segment_ids)\n",
    "        x = self.positional_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "# =======================\n",
    "# 2. Custom BERT Model Definition\n",
    "# =======================\n",
    "\n",
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, seq_len=128, num_layers=4, num_heads=4, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.embeddings = BERTEmbedding(vocab_size, d_model, seq_len)\n",
    "        \n",
    "        # Define a Transformer encoder (stack of encoder layers)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Masked Language Modeling head: project encoder output to the vocabulary space.\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, token_ids, segment_ids, attention_mask=None):\n",
    "        # token_ids, segment_ids: [batch_size, seq_len]\n",
    "        x = self.embeddings(token_ids, segment_ids)  # shape: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # PyTorch's Transformer encoder expects input shape: [seq_len, batch_size, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Create key padding mask if provided (mask positions where attention_mask==0)\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        # Pass through the encoder layers.\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        # Transform back to [batch_size, seq_len, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Compute MLM logits.\n",
    "        logits = self.mlm_head(x)  # shape: [batch_size, seq_len, vocab_size]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CustomBERT.forward() missing 1 required positional argument: 'segment_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m labels = input_ids.clone()\n\u001b[32m     39\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Compute loss: flatten logits and labels.\u001b[39;00m\n\u001b[32m     43\u001b[39m loss_fct = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: CustomBERT.forward() missing 1 required positional argument: 'segment_ids'"
     ]
    }
   ],
   "source": [
    "# Instantiate the custom tokenizer (assumed to be already trained and saved)\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "\n",
    "# Instantiate the custom BERT model.\n",
    "VOCAB_SIZE = 30000  # Should match the vocabulary size of your tokenizer.\n",
    "model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,\n",
    "                   num_layers=4, num_heads=4, hidden_dim=512)\n",
    "\n",
    "# Place the model on GPU if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop.\n",
    "num_epochs = 150\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        segment_ids = batch['segment_ids'].to(device)\n",
    "\n",
    "        # In a full MLM training scenario, you would apply a masking strategy.\n",
    "        # For this simplified example, we'll use input_ids as labels directly.\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(token_ids=input_ids, segment_ids=segment_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Compute loss: flatten logits and labels.\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "        loss = loss_fct(logits.view(-1, VOCAB_SIZE), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MaskedLMOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m     outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Extract the embedding from the [CLS] token\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m cls_embedding = \u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlast_hidden_state\u001b[49m[:, \u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# shape: [1, hidden_size]\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDocument embedding shape:\u001b[39m\u001b[33m\"\u001b[39m, cls_embedding.shape)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MaskedLMOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare a sample sentence with a masked token.\n",
    "# This example sentence contains a [MASK] token where we want the model to predict the missing word.\n",
    "test_sentence = \"Dette er et [MASK] eksempel på en setning.\"\n",
    "print(\"Test sentence:\", test_sentence)\n",
    "\n",
    "# Tokenize and encode the text, adding special tokens automatically.\n",
    "encoded = tokenizer.encode_plus(\n",
    "    test_sentence,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "\n",
    "input_ids = encoded[\"input_ids\"].to(device)\n",
    "attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "# Perform inference with no gradient tracking.\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    # 'outputs.logits' has shape [batch_size, seq_length, vocab_size]\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Find the position of the [MASK] token.\n",
    "mask_token_index = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "if len(mask_token_index[0]) == 0:\n",
    "    raise ValueError(\"No [MASK] token found in the input!\")\n",
    "# Assuming one [MASK] per input, obtain the index.\n",
    "mask_index = mask_token_index[1].item()\n",
    "\n",
    "# Get the predicted token for the masked position.\n",
    "predicted_token_id = predictions[0, mask_index].argmax(dim=-1).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
    "print(\"Predicted token:\", predicted_token)\n",
    "\n",
    "# Replace the [MASK] token in the original sentence.\n",
    "predicted_sentence = test_sentence.replace(\"[MASK]\", predicted_token)\n",
    "print(\"Predicted sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
