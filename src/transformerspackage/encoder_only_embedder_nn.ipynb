{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frist we need to load our dataset\n",
    "We have selected the NbAiLab/NCC dataset\n",
    "This is a dataset contructed by the national library optimized for training large language models\n",
    "We only want the nynorsk eksamples since our bert model are focusing on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "\n",
    "dataset = load_dataset(\"NbAiLab/NCC\", streaming=True)\n",
    "\n",
    "nynorsk_dataset = dataset[\"train\"].filter(lambda example: example.get(\"lang_fasttext\", \"\") == \"nn\")\n",
    "\n",
    "with open(\"nynorsk_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, example in enumerate(islice(nynorsk_dataset, 800)):\n",
    "        f.write(example[\"text\"].strip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'maalfrid_4b84113933f7134b94f68fbf804a70405b08e68e_23', 'doc_type': 'maalfrid_ssb', 'publish_year': 2021, 'lang_fasttext': 'nn', 'lang_fasttext_conf': '0.446', 'text': 'u) Nr ul c) u) o_ Nr Nr 001 0) 0 1,- 0) e-- CiD CO CD CD 0 Nt LO Nr Tr Co Tr ,- Nr c) c4 0) co_ 0) co r- Tr c» o Nr ul u) Tr co r- Nr a) Nr u)'}\n",
      "{'id': 'maalfrid_f1ebddd1220542416ef98935770372f0fc8c011b_61', 'doc_type': 'maalfrid_regjeringen', 'publish_year': 2021, 'lang_fasttext': 'nn', 'lang_fasttext_conf': '0.607', 'text': 'Boktittelen Språk er makt av psykologen Rolv Mik\\xad kel Blakar frå 1973 opna eit nytt perspektiv på språk og vart raskt eit munnhell. No er det også vorte vanleg å seia at språk er ei viktig kjelde til makt, eller at språk og språkbruk representerer makt. Språk og språkbruk er integrert i samfunnet, «der kontroll, påvirkning og makt er uttrykk for sentrale mellommenneskelege relasjonar», skriv Blakar. Men språk er ikkje alltid makt, og makt dreier seg om mykje meir enn språk og språkbruk. I det daglege gjev språk makt ved at nokon defi\\xad nerer situasjonar, markerer tilknyting eller avstand, gruppefellesskap eller sosialt overtak. Dette kan gjerast ut frå ein målretta og medviten strategi, eller utan at den som utøver makta, ten\\xad kjer over kva som skjer. Språkmakt kan koma frå språket sjølv, frå det å ha ordet i si makt, eller frå ytre omstende. Den makta som ligg i sjølve språket, har å gjera med både form og innhald i det som blir sagt eller skrive. Den makta som ligg utanfor språket, kjem sær\\xad leg frå den som ytrar seg, den gruppa vedkomande høyrer til, eller den status som språksystemet har. Mest språkmakt har gjerne den som både meistrar det språklege uttrykket og har ein sosial posisjon som gjev ekstra respekt eller truverde. Det som er tema i denne meldinga, er språk og språkbruk i det offentlege rommet, utanfor privat\\xad livet og familiens fire vegger. Vi ser bort frå den typen uformell språkleg makt som ligg i sjølve språksystemet og i det å meistra språklege verke\\xad middel, med mindre slik makt har eller kan få føl\\xad gjer for samfunnsutforminga og politiske avgjer\\xad der. Den makta som ligg i språket, kjem her i bak\\xad grunnen for den språklege makta som ligg i det å kunna bruka språket og delta i dei demokratiske prosessane i samfunnet. Drøftinga i dette kapitlet er ordna etter ulike sider i tilhøvet mellom makt og språk. Forholdet mellom dei to offisielle målformene er både formelt og språkleg annleis enn forholdet mellom norsk og samisk, norsk og minoritetsspråk, norsk og engelsk. Men ved at vi her i prinsippet drøftar makt på tvers av slike språkskilje, kan både felles\\xad trekk og skilnader bli tydelegare. Dette kan gje ei 3 Blakar, Rolv Mikkel 2006: 225. meir samla forståing av forholdet mellom makt, språk og demokrati i Noreg. I Noreg er det gjennomført to store maktutgreiin\\xad gar. Den første vart gjennomført under leiing av Gudmund Hernes i åra 1972–82. Den andre, under leiing av Øyvind Østerud i perioden 1998–2003, er også nemnd ovanfor. Den første utgreiinga skulle «skaffe til veie et best mulig kjennskap til de reelle maktforhold i det norske samfunn». Ut frå ei forståing av «maktut\\xad øvelse som viljesutøvelse» var utgreiinga konsent\\xad rert om fordeling av økonomisk og politisk makt, og om forholdet mellom statsmakt og marknads\\xad makt. I sluttrapporten vart den makt massemedia har, drøfta i eit lengre kapittel, men ikkje forholdet mel\\xad lom makt og språk. Hernes skreiv i det teoretiske grunnlagsdokumentet Makt og avmakt eit avsnitt som dels bygde på framstillinga til Blakar. Der hei\\xad ter det: «Språket har makt over tanken fordi det gjør det vanskeligere å tenke i andre baner. Språket kan også brukes til å utelukke andre fra forstå\\xad else, og dermed fra meningsberettigelse og deltakelse. Det språk som er knyttet til flere profesjoner tar til dels sikte på at diskusjonen mellom deres utøvere skal gå over hodene på folk slik at de ikke blander seg inn.» Ei deloppgåve i mandatet var å «klarlegge grup\\xad per som har liten innflytelse over sin egen situa\\xad sjon», men dette vart i liten grad følgt opp i dei mange bøkene og skriftene frå denne første norske maktutgreiinga. Perspektiva på makt vart utvida i 1980- og 90\\xad åra, ikkje minst ved at søkjelyset vart retta mot kva rolle kommunikasjon og språk har i samfunnet. Dette viser att i den andre maktutgreiinga. Hovud\\xad temaet i dette utgreiingsarbeidet var «vilkårene for det norske folkestyret og endringer i disse». Her skulle ein mellom anna vurdera kva sosiale, økono\\xad miske og kulturelle skiljelinjer, alder og kjønn kunne ha å seia for høve til å delta og påverka. Utgreiinga skulle på den eine sida avgrensa merk\\xad semda til forholdet mellom makt og demokrati, og på den andre sida drøfta dette frå så mange syns\\xad vinklar at den samla drøftinga ville bli ganske mykje vidare enn i den første maktutgreiinga. Sluttrapporten er NOU 2003:19 Makt og demo\\xad krati. I tillegg kom det 76 skrifter i ein rapportserie, 51 boktitlar og store mengder fagartiklar og kro\\xad 4 Petersson, Olof 1987:'}\n",
      "{'id': 'maalfrid_fcc34148f00916730145e534fe2e41f97089193d_12', 'doc_type': 'maalfrid_nlr', 'publish_year': 2021, 'lang_fasttext': 'nn', 'lang_fasttext_conf': '0.461', 'text': 'Skjellanger, Meland Beiteområdet til Skjellanger beitelag ligg nord i Meland kommune, mellom utmarksgrensa til Husebø i søraust og Skjellanger fort i nordvest. Arealet er kystlynghei med innslag av myr, furuskog, einer og plantefelt med gran. Folk som bur her, nyttar området til å gå turar. To av bøndene har gått saman om skjøtsel og beiting av arealet, dei har stifta beitelag og utarbeidd beitebruksplan. Tal bruk 7 Tal bønder 2 Dyreslag (tal) Storfe (7), sau (20), nokon geit Areal 900 da Spesielle verdiar Kystlynghei, friluftsområde Tilskot SMIL (2005): 90 000 kr RMP (2005): 10 000 kr Tiltak i beitebruksplanen:'}\n",
      "{'id': 'bruvik_null_null_19710430_20_16_1_MODSMD_ARTICLE6', 'doc_type': 'newspaper_ocr', 'publish_year': 1971, 'lang_fasttext': 'nn', 'lang_fasttext_conf': '0.677', 'text': 'hadde årsmøte i Bergen med helga. Det vart bore fram helsingar frå Folkeakademienes Landsfor bund ved Arna Folkedal og frå Hordaland Fylkesopplysinngsråd ved Johs. Grasdal. Kontaktutvalet for populær vitskapleg verksemd ved Univer sitetet i Bergen var representert ved formannen, dosent Egil Bak ka, som heldt foredrag om «Kris tendom i vikingebygdene i vest». og gjorde greie for tilbod om føre lesarar og kursemne frå Univer sitetet. Av årsmeldinga gjekk det fram at aktiviteten hadde vore større enn nokon gong før. 26 akademi hadde gjeve melding om 370 til skipingar med tils. 32 442 fram motte (tala frå året for var: 322 tilskipingar og 29 172 frammøtte). Av dette var det 8 turnear med 97 tilskipingar. Det største arrangementet var Vigelandsutstillinga, som vart vitja av vel 2500 personar. I høve Naturvernåret 1970 kan | nemnast emna: «Kjemi, biologi og naturvern», «Naturvern, helse og politikk», «Mennesket i natur en», «Biologi og menneskeverd», i«Kjeldevatn kloakk», «Indu striområde bustadfelt». Litterære emne var: «Glytt inn i diktarverkstaden», «Arets litteratur», «Dans ropte fela», ! «Vagantviser», «Julesongar, jule ! opptøy», «Biblioteket og studie ! arbeidet». «Vaksenopplæring» var I også eit aktuelt emne. drag om «Ungdom og medika mentmisbruk-», «Narkotikapro blemet», «Abortproblemet» og «Hvordan kan vi bygge opp bar nekroppen». Andre emne var: «Korleis levde fedre våre?» og «FN idag». Desse musikkprogram vart gjevne : Ingebjørg Gresvik —S. Torstenson: Edvard Grieg, Lasse Kolstad Tor Hultin: Viser, Liv Dommersnes Liv Glaser: Ly rikk musikk, Ruth Reese: Neg ro Spirituals. Elles er det i stor mon nytta lokale krefter. Det kan trygt seiast at mange akademi har gjeve eit rikt og mangesidig tilbod. Program med tilknyting til det lokale bygde samfunnet har samla mykje folk. Ei rekkje akademi står som til skiparar på det lokale plan for Riksgalleriet og Riksteatret. Den mangeårige leiaren av akademiverksemda i Hordaland, Svein Simonsen, vart varmt tak ka og hylla av årsmøtet. Styret for Hordaland Folkeaka demi er: Arna Folkedal, Ulvik, (varaform.) Johs. Grasdal, Stord. Svein Simonsen, Arna og K. N Hjeltnes, Kvam (form.)'}\n",
      "{'id': 'firdafolkeblad_null_null_19400423_35_32_1_MODSMD_ARTICLE8', 'doc_type': 'newspaper_ocr', 'publish_year': 1940, 'lang_fasttext': 'nn', 'lang_fasttext_conf': '0.563', 'text': 'Kampane sundag. Hærens overkommando mel der den 22. april : Den 21. april heldt tyskarane fram med sine åtak på stillingane våre på båe sidor av Mjøsa stotta av artilleri, stridsvogner Øog fly. Tyskarane gjekk Øog på stillingane våre i Øysterdalen. I Valdres gjekk våre tropper fram Øog tok 200 fangar. Tyske fly bomoa norske sam bandslinor på fleire stader utan nemnande brot i trafikken. Tys karane kasta Øog ned flygeblad. 5 tyske fly vart nenskotne. På tyske fangar finn ein patroner der pro_ jektiler verkar som dum-dum.'}\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"NbAiLab/NCC\", split=\"validation\", streaming=True)\n",
    "\n",
    "test_nynorsk_dataset = test_dataset.filter(lambda example: example.get(\"lang_fasttext\", \"\") == \"nn\")\n",
    "\n",
    "for example in islice(test_nynorsk_dataset, 5):\n",
    "    print(example)\n",
    "\n",
    "with open(\"nynorsk_test_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, example in enumerate(islice(test_nynorsk_dataset, 800)):\n",
    "        f.write(example[\"text\"].strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and settin special tokens\n",
    "Before we start creating hour BERT model we first define our tokenizer. The tokenizer builds a tokenizermodel working with a set vocabulary size.\n",
    "\n",
    "It is also to define some special tokens. These tokens each have different responsibilities:\n",
    "\n",
    "-  [PAD] tokens are added to sentences so all of them have equal length\n",
    "-  [CLS] tokens are Classification tokens, it serves as a Start of sentence(SOS) and represent the meaning of the sentence\n",
    "- [SEP] represents the End of entence(EOS) and also separation between sentences\n",
    "- [MASK] is used for wordreplacement during masked language tasks\n",
    "- [UNK] is used for filling in for words that are out of the vocabulary og the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "tokenizer.train(\n",
    "    files=[\"nynorsk_corpus.txt\"],\n",
    "    vocab_size=10000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
    ")\n",
    "\n",
    "tokenizer.save_model(\"tokenizer/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just a small script for loading in the tokenizer model from a directory\n",
    "Now we do not need to train it every time we want to test something quickly. Yippieee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Example tokens: ['dette', 'er', 'ein', 'test', 'for', 'nynorsk', '.']\n",
      "Vocabulary size: 10000\n",
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "# load_tokenizer.py\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "print(\"Tokenizer loaded. Example tokens:\", tokenizer.tokenize(\"Dette er ein test for Nynorsk.\"))\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCCTorchDataset\n",
    "Before sending our dataset into a BERT model we need to prepare it.\n",
    "In this class we use the tokenizer we trained earlier to encode the text included in the dataset.\n",
    "\n",
    "Here an important feature to mention is the DataCollectorForLanguageModdeling class. This masks a procentage of the dataset so it will be prime to use for training a BERT model for mlm tasks. For each batch that are pulled from the dataloader a new masking pattern is created, securing that there are differences in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating alle split: 800 examples [00:00, 82094.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch shapes:\n",
      "  input_ids      : (32, 128)\n",
      "  attention_mask : (32, 128)\n",
      "  token_type_ids : (32, 128)\n",
      "  labels         : (32, 128)\n",
      "\n",
      "Dev batch shapes:\n",
      "  input_ids      : (32, 128)\n",
      "  attention_mask : (32, 128)\n",
      "  token_type_ids : (32, 128)\n",
      "  labels         : (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "class NCCTorchDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer: BertTokenizer, max_len=128):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.max_len    = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.hf_dataset[index]['text']\n",
    "        enc  = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids      = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        token_type_ids = torch.zeros_like(input_ids, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "hf_dataset = load_dataset(\"text\", data_files={\"alle\": \"nynorsk_corpus.txt\"}, split=\"alle\")\n",
    "split = hf_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_hf = split['train']\n",
    "dev_hf   = split['test']\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "\n",
    "train_ds = NCCTorchDataset(train_hf, tokenizer, max_len=128)\n",
    "\n",
    "dev_ds = NCCTorchDataset(dev_hf, tokenizer, max_len=128)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Train batch shapes:\")\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    print(f\"  {k:15s}: {tuple(v.shape)}\")\n",
    "\n",
    "print(\"\\nDev batch shapes:\")\n",
    "batch = next(iter(dev_loader))\n",
    "for k, v in batch.items():\n",
    "    print(f\"  {k:15s}: {tuple(v.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the good stuff\n",
    "In the following cell we have prepared classes for positional embedding, BERT embedding and a CustomBERT model that we are going to train later. \n",
    "\n",
    "## Postitional embedding\n",
    "\n",
    "Positional embedding is the first part of the pipeline. It is a clever way of to destingush where in a sequence a word is placed, and also the distance to other words in the sequence. Sine and cosine functions are used to create unique positional encodings for each position in the sequence\n",
    "\n",
    "## BERT embedding\n",
    "The BERTembedding class is used to create input embeddings for the BERT model\n",
    "It combines token embeddings, segment embeddings and positional embeddings.\n",
    "The token embeddings are learned from the vocabularym while the segment embeddings are used tp diffriencate between two segments in the input. The positional embeddings are added to the token embeddings to provide information about the position of each token. Dropout layer is to prevent overfitting...\n",
    "\n",
    "# CustomBERT\n",
    "The CustomBERT class is a simplified version of the BERT model.\n",
    "It consists of an embedding layer, a transformer encoder, and a masked language modeling (MLM) head.\n",
    "The transformer encoder is composed of multiple layers, each containing multi-head self-attention\n",
    "and feedforward neural networks.\n",
    "The MLM head is used to predict the masked tokens in the input sequence.\n",
    "The forward method takes token IDs, segment IDs, and an optional attention mask as input,\n",
    "and returns the logits for the masked language modeling task.\n",
    "The save_model method allows saving the model's state dictionary to a specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.segment_embedding = nn.Embedding(2, d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(d_model, seq_len)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, token_ids, segment_ids):\n",
    "        x = self.token_embedding(token_ids) + self.segment_embedding(segment_ids)\n",
    "        x = self.positional_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, seq_len=128, num_layers=4, num_heads=4, dim_feedforward=512):\n",
    "        super().__init__()\n",
    "        self.embeddings = BERTEmbedding(vocab_size, d_model, seq_len)        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Masked Language Modeling head: project encoder output to the vocabulary space.\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, token_ids, segment_ids, attention_mask=None):\n",
    "        # token_ids, segment_ids: [batch_size, seq_len]\n",
    "        x = self.embeddings(token_ids, segment_ids)  # shape: [batch_size, seq_len, d_model]\n",
    "        # PyTorch's Transformer encoder expects input shape: [seq_len, batch_size, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        # Create key padding mask if provided (mask positions where attention_mask==0)\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None \n",
    "        # Pass through the encoder layers.\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        # Transform back to [batch_size, seq_len, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        # Compute MLM logits.\n",
    "        logits = self.mlm_head(x)  # shape: [batch_size, seq_len, vocab_size]\n",
    "        return logits\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we train our own customBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following steps are used\n",
    "\n",
    "Create a model and what hyperparameters it should have.\n",
    "\n",
    "Optimizer\n",
    "We use an adamW optimizer for updating the models parameters based on gradients computed during back propagation\n",
    "\n",
    "Loss function\n",
    "The loss function is a crossentropu loss that computes the difference between our models predicted token distribution, and the \"true\" tokens it want it to predikt\n",
    "by setting all unmasked positions in the labels to -100, we kan ignore index -100 to skip those positions when averaging the loss. This penalizes the model for getting masked tokens wrong, matching BERT mlm objective\n",
    "\n",
    "GradScaler\n",
    "Then we use a gradient scaler. This utilizes 16.bit operations which uses a lott less memory than the 32 bit operations. CHATGPT said this will help so we tried it\n",
    "\n",
    "## Then the epochs\n",
    "\n",
    "each epoch loops trhough the whole dataset once. The first lines load the tensors from the batch. Then zero_grad() is added to clear gradient buffers from previous steps.\n",
    "Autocast enables mixed-precision, and reshapes the tensors from [B*L,V] whre B are batch size, L sequence lenght and V the Vocab_length, down to [B*L] this makes it possible for CrossEntropyLoss to compute the average negative log-likelygood over the masked tokens\n",
    "\n",
    "Backpropagation is done with the gradscaler to correct the model taking the los as an input\n",
    "\n",
    "These teqniques were not used earlier for some models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [train]:   0%|          | 0/23 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "model = CustomBERT(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=768,\n",
    "    seq_len=128,\n",
    "    num_layers=4,\n",
    "    num_heads=12,\n",
    "    dim_feedforward=3072\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs   = 3\n",
    "total_steps  = num_epochs * len(train_loader)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler    = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "scaler  = torch.GradScaler(\"cpu\")\n",
    "\n",
    "best_dev_loss = float(\"inf\")\n",
    "\n",
    "print(\"Training started...\\n\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # — Training —\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch} [train]\", unit=\"batch\")\n",
    "    for step, batch in enumerate(train_loop, start=1):\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(\"cpu\"):\n",
    "            logits = model(\n",
    "                token_ids      = input_ids,\n",
    "                segment_ids    = token_type_ids,\n",
    "                attention_mask = attention_mask\n",
    "            )\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, VOCAB_SIZE),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        train_loop.set_postfix(train_loss=f\"{running_train_loss/step:.4f}\")\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    print(f\"\\n→ Epoch {epoch} Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # — Evaluation on Dev —\n",
    "    model.eval()\n",
    "    running_dev_loss = 0.0\n",
    "    dev_loop = tqdm(dev_loader, desc=f\"Epoch {epoch} [dev]  \", unit=\"batch\")\n",
    "\n",
    "    for step, batch in enumerate(dev_loop, start=1):\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(\n",
    "                token_ids      = input_ids,\n",
    "                segment_ids    = token_type_ids,\n",
    "                attention_mask = attention_mask\n",
    "            )\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, VOCAB_SIZE),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "            running_dev_loss += loss.item()\n",
    "\n",
    "        dev_loop.set_postfix(dev_loss=f\"{running_dev_loss/step:.4f}\")\n",
    "\n",
    "    avg_dev_loss = running_dev_loss / len(dev_loader)\n",
    "    print(f\"Epoch {epoch} Dev   Loss: {avg_dev_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # — Save best model —\n",
    "    if avg_dev_loss < best_dev_loss:\n",
    "        best_dev_loss = avg_dev_loss\n",
    "        torch.save(model.state_dict(), \"custom_bert_model/best_mlm_model.pth\")\n",
    "        print(\"  * New best dev loss, model saved.\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 predictions for [MASK]: ['.', 'i', \"'\", ',', '##ד', '##el', '##ed', '##in', '##et', '##en']\n",
      "Original sentence: Dette er ein [MASK] for Nynorsk.\n",
      "Masked token ID: 18\n",
      "Masked token: [MASK]\n",
      "Prediction for [MASK]: .\n",
      "Dette er ein . for Nynorsk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer  # pip install transformers\n",
    "\n",
    "# 1) Load tokenizer & model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "# Model trained on 500 examples with a VOCAB_SIZE of 10000, and tuned parameters, + masked data and backpropagation.\n",
    "model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=4, num_heads=12, dim_feedforward==3072)\n",
    "\n",
    "\n",
    "# Load the custom BERT model with tuned parameters trained on 2000 examples.\n",
    "#model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=12, num_heads=12, dim_feedforward=512)\n",
    "\n",
    "\n",
    "# Load model tuned with parameters trained on 8000 examples.\n",
    "# model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "\n",
    "# Load model with smaller parameters trainied on 10000 examples.\n",
    "# model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "\n",
    "\n",
    "# Load model trained on 100000 examples using the same parameters as the small, with a gpu.\n",
    "#model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "\n",
    "# Load model from directory.\n",
    "current_model = \"mlm_finetuned.pth\"\n",
    "checkpoint = torch.load(\n",
    "    \"custom_bert_model/\" + current_model,\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# 2) Prepare a masked sentence\n",
    "sentence = \"Dette er ein [MASK] for Nynorsk.\"\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "token_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "segment_ids = torch.zeros_like(token_ids)\n",
    "\n",
    "# 3) Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(token_ids, segment_ids, attention_mask)\n",
    "\n",
    "# 4) Find masked position(s) and predict\n",
    "mask_token_index = (token_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "batch_idx, seq_pos = mask_token_index[0][0], mask_token_index[1][0]\n",
    "mask_logits = logits[batch_idx, seq_pos]\n",
    "# Get the 5 most likely tokens for the masked position.\n",
    "top_k = 5\n",
    "top_k_indices = torch.topk(mask_logits, top_k).indices\n",
    "top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices.tolist())\n",
    "# Print the top-k predictions\n",
    "print(f\"Top {top_k} predictions for [MASK]: {top_k_tokens}\")\n",
    "# 5) Get the predicted token ID and convert it to a token\n",
    "predicted_id = mask_logits.argmax(dim=-1).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_id)\n",
    "# 5) Print the prediction\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Masked token ID: {predicted_id}\")\n",
    "print(f\"Masked token: {tokenizer.mask_token}\")\n",
    "print(f\"Prediction for [MASK]: {predicted_token}\")\n",
    "print(f\"Dette er ein {predicted_token} for Nynorsk.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try to score the models on classification\n",
    "We will wrap the CustomBERT in a BertForSequenceClassification\n",
    "\n",
    "Then we gather all preds and all labels as numpy arrays\n",
    "\n",
    "for metrics we create a classifivation_report that shows precision/recall/F! scores\n",
    "\n",
    "end we will visualize using a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBert Evaluation results on dev set:\n",
      "  avg_loss: 11.1442\n",
      "  perplexity: 69164.2232\n",
      "  top1_accuracy: 0.0635\n",
      "  top5_accuracy: 0.0642\n",
      "\n",
      " Custombert2 Evaluation results on dev set:\n",
      "  avg_loss: 10.5152\n",
      "  perplexity: 36871.4029\n",
      "  top1_accuracy: 0.0430\n",
      "  top5_accuracy: 0.0909\n",
      "\n",
      "MLM Finetuned Evaluation results on dev set:\n",
      "  avg_loss: 8.4661\n",
      "  perplexity: 4750.8034\n",
      "  top1_accuracy: 0.0395\n",
      "  top5_accuracy: 0.1118\n",
      "\n",
      "I1 CustomBert Evaluation results on dev set:\n",
      "  avg_loss: 12.4231\n",
      "  perplexity: 248486.2151\n",
      "  top1_accuracy: 0.0536\n",
      "  top5_accuracy: 0.0536\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict\n",
    "\n",
    "def evaluate_mlm(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    vocab_size: int,\n",
    "    device: torch.device\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a masked language model on a dataloader.\n",
    "\n",
    "    Returns a dictionary with:\n",
    "      - avg_loss: float\n",
    "      - perplexity: float\n",
    "      - top1_acc: float\n",
    "      - top5_acc: float\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_masked = 0\n",
    "    correct1 = 0\n",
    "    correct5 = 0\n",
    "    vocab_size = vocab_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids      = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                token_ids = input_ids,\n",
    "                segment_ids = token_type_ids,\n",
    "                attention_mask = attention_mask\n",
    "            )  # [B, L, V]\n",
    "\n",
    "            # Compute loss over masked tokens\n",
    "            loss = loss_fn(logits.view(-1, vocab_size), labels.view(-1))\n",
    "            num_masked = (labels.view(-1) != -100).sum().item()\n",
    "            total_loss += loss.item() * num_masked\n",
    "            total_masked += num_masked\n",
    "\n",
    "            # Compute top-k accuracy\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            mask_pos = labels != -100\n",
    "            masked_preds = probs[mask_pos]  # [N_masked, V]\n",
    "            true_ids = labels[mask_pos]     # [N_masked]\n",
    "\n",
    "            # Top-1\n",
    "            pred1 = masked_preds.argmax(dim=-1)\n",
    "            correct1 += (pred1 == true_ids).sum().item()\n",
    "\n",
    "            # Top-5\n",
    "            top5 = torch.topk(masked_preds, k=5, dim=-1).indices  # [N_masked, 5]\n",
    "            for i in range(top5.size(0)):\n",
    "                if true_ids[i].item() in top5[i].tolist():\n",
    "                    correct5 += 1\n",
    "\n",
    "    avg_loss = total_loss / total_masked\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    top1_acc = correct1 / total_masked\n",
    "    top5_acc = correct5 / total_masked\n",
    "\n",
    "    return {\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"top1_accuracy\": top1_acc,\n",
    "        \"top5_accuracy\": top5_acc\n",
    "    }\n",
    "\n",
    "custom_bert1 = CustomBERT(vocab_size=30000, d_model=256, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "checkpoint = torch.load(\n",
    "    \"custom_bert_model/custom_bert_model.pth\",\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "custom_bert1.load_state_dict(checkpoint)\n",
    "custom_bert1.to(device)\n",
    "custom_bert1.eval()\n",
    "\n",
    "custom_bert2 = CustomBERT(vocab_size=30000, d_model=768, seq_len=128,num_layers=12, num_heads=12, dim_feedforward=512)\n",
    "checkpoint = torch.load(\n",
    "    \"custom_bert_model/custom_bert_model2.pth\",\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "custom_bert2.load_state_dict(checkpoint)\n",
    "custom_bert2.to(device)\n",
    "custom_bert2.eval()\n",
    "\n",
    "# Evaluate the models\n",
    "dev_results = evaluate_mlm(\n",
    "    model=custom_bert1,\n",
    "    dataloader=dev_loader,\n",
    "    vocab_size=30000,\n",
    "    device=device\n",
    ")\n",
    "print(\"CustomBert Evaluation results on dev set:\")\n",
    "for key, value in dev_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "cb_2_results = evaluate_mlm(\n",
    "    model=custom_bert2,\n",
    "    dataloader=dev_loader,\n",
    "    vocab_size=30000,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n Custombert2 Evaluation results on dev set:\")\n",
    "for key, value in cb_2_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "mlmf_results = evaluate_mlm(\n",
    "    model=model,\n",
    "    dataloader=dev_loader,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    device=device\n",
    ")\n",
    "print(\"\\nMLM Finetuned Evaluation results on dev set:\")\n",
    "for key, value in mlmf_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "#model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "# Load model from directory.\n",
    "i1_bert = CustomBERT(vocab_size=30000, d_model=256, seq_len=128,num_layers=4, num_heads=4, dim_feedforward=512)\n",
    "checkpoint = torch.load(\n",
    "    \"custom_bert_model/i1_custom_bert_model.pth\",\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "i1_bert.load_state_dict(checkpoint)\n",
    "i1_bert.to(device)\n",
    "i1_bert.eval()\n",
    "\n",
    "# Evaluate the models   \n",
    "i1_results = evaluate_mlm(\n",
    "    model=i1_bert,\n",
    "    dataloader=dev_loader,\n",
    "    vocab_size=30000,\n",
    "    device=device\n",
    ")\n",
    "print(\"\\nI1 CustomBert Evaluation results on dev set:\")\n",
    "for key, value in i1_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets train a predefined bert model from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch import optim\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "import random\n",
    "\n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling.\n",
    "    This function randomly masks tokens with a probability of mlm_probability.\n",
    "    \"\"\"\n",
    "    labels = inputs.clone()\n",
    "    # Create a mask of positions to mask\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "        for val in labels.tolist()\n",
    "    ]\n",
    "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # Only compute loss on masked tokens\n",
    "\n",
    "    # Replace masked indices with the [MASK] token id\n",
    "    inputs[masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    return inputs, labels\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # Move inputs to device:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['segment_ids'].to(device)\n",
    "\n",
    "        # Apply masking for MLM:\n",
    "        masked_input_ids, labels = mask_tokens(input_ids.clone(), tokenizer, mlm_probability=0.15)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=masked_input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels.to(device)\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomBERT' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m os.makedirs(directory, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mfinetuned_bert_model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m(os.path.join(directory, \u001b[33m\"\u001b[39m\u001b[33mBERTmlm.pth\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'CustomBERT' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "# Save the model after training.\n",
    "directory = \"custom_bert_model/\"\n",
    "import os\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "model_name = \"finetuned_bert_model\"\n",
    "model.save_pretrained(os.path.join(directory, \"BERTmlm.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while deserializing header: MetadataIncompleteBuffer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model_dir = \u001b[33m\"\u001b[39m\u001b[33mcustom_bert_model/BERTmlm\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m tokenizer_dir = \u001b[33m\"\u001b[39m\u001b[33mtokenizer/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mBertForMaskedLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m tokenizer = BertTokenizer.from_pretrained(tokenizer_dir)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Create the fill-mask pipeline\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:272\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    274\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4345\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4337\u001b[39m is_from_file = pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4340\u001b[39m     is_safetensors_available()\n\u001b[32m   4341\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_from_file\n\u001b[32m   4342\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded\n\u001b[32m   4343\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m checkpoint_files[\u001b[32m0\u001b[39m].endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4344\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m4345\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   4346\u001b[39m         metadata = f.metadata()\n\u001b[32m   4348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4349\u001b[39m         \u001b[38;5;66;03m# Assume it's a pytorch checkpoint (introduced for timm checkpoints)\u001b[39;00m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while deserializing header: MetadataIncompleteBuffer"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load your fine-tuned model and your custom tokenizer\n",
    "model_dir = \"custom_bert_model/BERTmlm\"\n",
    "tokenizer_dir = \"tokenizer/\"\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "# Create the fill-mask pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a test sentence with a [MASK] token.\n",
    "# (Make sure the special mask token in your tokenizer matches what you use here.)\n",
    "test_sentence = \"Dette er et eksempel på [MASK] norsk setning.\"\n",
    "results = fill_mask(test_sentence)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Prediction: {result['token_str']}, Score: {result['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector arithmetic result: dag + seint yields:\n",
      "1: seint (cosine similarity: 0.7218)\n",
      "2: dag (cosine similarity: 0.6949)\n",
      "3: ##nende (cosine similarity: 0.2359)\n",
      "4: admin (cosine similarity: 0.2355)\n",
      "5: ferdene (cosine similarity: 0.2219)\n",
      "6: ##snemnda (cosine similarity: 0.2189)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Assume CustomBERT is defined in your code (see previous examples)\n",
    "# from custom_bert import CustomBERT\n",
    "\n",
    "# Load the custom tokenizer and model.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "# model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=12, num_heads=12, hidden_dim=512)\n",
    "model = CustomBERT(vocab_size=30000, d_model=256, seq_len=128, num_layers=4, num_heads=4, hidden_dim=512)\n",
    "model.load_state_dict(torch.load(\"custom_bert_model/i1_custom_bert_model.pth\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_token_vector(word, tokenizer, model):\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    if not tokens:\n",
    "        raise ValueError(f\"Word '{word}' could not be tokenized.\")\n",
    "    token_id = tokenizer.convert_tokens_to_ids(tokens[0])\n",
    "    return model.embeddings.token_embedding.weight[token_id]\n",
    "\n",
    "word_a = \"dag\"\n",
    "word_b = \"seint\"\n",
    "\n",
    "vec_a = get_token_vector(word_a, tokenizer, model)\n",
    "vec_b = get_token_vector(word_b, tokenizer, model)\n",
    "\n",
    "target_vector = vec_a + vec_b\n",
    "\n",
    "# Compute cosine similarities with all token embeddings.\n",
    "all_embeddings = model.embeddings.token_embedding.weight\n",
    "target_vector_norm = F.normalize(target_vector.unsqueeze(0), dim=-1)\n",
    "all_embeddings_norm = F.normalize(all_embeddings, dim=-1)\n",
    "cosine_sim = torch.matmul(target_vector_norm, all_embeddings_norm.transpose(0, 1))\n",
    "\n",
    "# Retrieve the top 3 tokens with highest cosine similarity.\n",
    "topk = torch.topk(cosine_sim, k=6)\n",
    "top_values = topk.values.squeeze(0).tolist()\n",
    "top_indices = topk.indices.squeeze(0).tolist()\n",
    "\n",
    "print(f\"Vector arithmetic result: {word_a} + {word_b} yields:\")\n",
    "for i, (score, idx) in enumerate(zip(top_values, top_indices)):\n",
    "    token = tokenizer.convert_ids_to_tokens(idx)\n",
    "    print(f\"{i+1}: {token} (cosine similarity: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
