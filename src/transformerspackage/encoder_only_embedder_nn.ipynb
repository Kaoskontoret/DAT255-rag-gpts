{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nynorsk sentences: 50000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "def load_nynorsk_sentences(limit=50000):\n",
    "    dataset = load_dataset(\"NbAiLab/NCC\", streaming=True)\n",
    "    train_stream = dataset[\"train\"]\n",
    "    sentences = []\n",
    "    for example in train_stream:\n",
    "        if example.get(\"lang_fasttext\") == \"nn\":\n",
    "            text = example[\"text\"]\n",
    "            for s in re.split(r\"[.!?]\\\\s+\", text):\n",
    "                if len(s.split()) > 3:\n",
    "                    sentences.append(s.strip())\n",
    "        if len(sentences) >= limit:\n",
    "            break\n",
    "    return sentences\n",
    "\n",
    "nynorsk_sentences = load_nynorsk_sentences()\n",
    "print(\"Number of Nynorsk sentences:\", len(nynorsk_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nynorsk validation sentences: 10966\n"
     ]
    }
   ],
   "source": [
    "def load_validation_sentences():\n",
    "    dataset = load_dataset(\"NbAiLab/NCC\", split=\"validation\")\n",
    "    sentences = []\n",
    "    for example in dataset:\n",
    "        if example.get(\"lang_fasttext\") == \"nn\":\n",
    "            text = example[\"text\"]\n",
    "            for s in re.split(r\"[.!?]\\\\s+\", text):\n",
    "                if len(s.split()) > 3:\n",
    "                    sentences.append(s.strip())\n",
    "    return sentences\n",
    "nynorsk_validation_sentences = load_validation_sentences()\n",
    "print(\"Number of Nynorsk validation sentences:\", len(nynorsk_validation_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "VOCAB_SIZE = 12582912\n",
    "N_SEGMENTS = 3\n",
    "MAX_LENGTH = 512\n",
    "EMBEDDING_DIM = 768\n",
    "N_LAYERS = 12\n",
    "ATTN_HEADS = 12\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NorwegianDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_length=512):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Squeeze to remove extra dimension (batch dim inside each sample)\n",
    "        encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        return encoding\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "dataset = NorwegianDataset(nynorsk_sentences, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, n_segments, max_length, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.segment_embedding = nn.Embedding(n_segments, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length, embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.pos_input = torch.tensor([[i for i in range(max_length)]])\n",
    "\n",
    "    def forward(self, seq, seg):\n",
    "        embed_values = self.token_embedding(seq) + self.segment_embedding(seg) + self.position_embedding(self.pos_input)\n",
    "        embed_values = self.dropout(embed_values)\n",
    "        return embed_values\n",
    "    \n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_segments, max_length, embedding_dim, n_layers, attn_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.embedding = BERTEmbedding(vocab_size, n_segments, max_length, embedding_dim, dropout_rate)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embedding_dim, attn_heads, embedding_dim * 4, dropout=dropout_rate, activation=\"gelu\")\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, n_layers)\n",
    "\n",
    "    def forward(self, seq, seg):\n",
    "        out = self.embedding(seq, seg)\n",
    "        out = self.encoder_block(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTMLM(nn.Module):\n",
    "    def __init__(self, bert_model, vocab_size):\n",
    "            super().__init__()\n",
    "            self.bert = bert_model\n",
    "            self.mlm_head = nn.Linear(bert_model.embedding.token_embedding.embedding_dim, vocab_size)\n",
    "            \n",
    "    def forward(self, seq, seg):\n",
    "        hidden_states = self.bert(seq, seg)\n",
    "        # Assuming you want to predict for every token\n",
    "        prediction_scores = self.mlm_head(hidden_states)\n",
    "        return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    labels = inputs.clone()\n",
    "    # Create a mask for positions to mask\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    \n",
    "    # Only compute loss on masked tokens\n",
    "    labels[~masked_indices] = -100  # Using -100 as ignore index\n",
    "    \n",
    "    # 80% replace with [MASK]\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    \n",
    "    # 10% replace with random token\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "    \n",
    "    # The rest 10% keep unchanged (masked_indices & not replaced by mask or random)\n",
    "    return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTMLM(BERT(VOCAB_SIZE, N_SEGMENTS, MAX_LENGTH, EMBEDDING_DIM, N_LAYERS, ATTN_HEADS, DROPOUT_RATE), VOCAB_SIZE)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "num_epochs = 3  # Adjust as needed\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # Move input tensors to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        # Use segment ids if available; otherwise, create a dummy tensor\n",
    "        token_type_ids = batch.get('token_type_ids', torch.zeros_like(input_ids)).to(device)\n",
    "\n",
    "        # Apply the MLM masking function\n",
    "        inputs_masked, labels = mask_tokens(input_ids.clone(), tokenizer)\n",
    "        inputs_masked = inputs_masked.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs_masked, token_type_ids)\n",
    "        \n",
    "        # Compute loss; reshape to (batch_size*seq_length, vocab_size)\n",
    "        loss = loss_fn(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch} Loss: {loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
