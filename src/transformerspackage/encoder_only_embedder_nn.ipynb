{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frist we need to load our dataset\n",
    "We have selected the NbAiLab/NCC dataset\n",
    "This is a dataset contructed by the national library optimized for training large language models\n",
    "We only want the nynorsk eksamples since our bert model are focusing on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "\n",
    "# Load the dataset in streaming mode.\n",
    "dataset = load_dataset(\"NbAiLab/NCC\", streaming=True)\n",
    "\n",
    "# Use the 'train' split from the dataset.\n",
    "# Note: Make sure to filter on the correct language field.\n",
    "nynorsk_dataset = dataset[\"train\"].filter(lambda example: example.get(\"lang_fasttext\", \"\") == \"nn\")\n",
    "\n",
    "# Write a subset (e.g., first 1000 examples) to a file using islice.\n",
    "with open(\"nynorsk_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, example in enumerate(islice(nynorsk_dataset, 100000)):\n",
    "        f.write(example[\"text\"].strip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and settin special tokens\n",
    "Before we start creating hour BERT model we first define our tokenizer. The tokenizer builds a tokenizermodel working with a set vocabulary size.\n",
    "\n",
    "It is also to define some special tokens. These tokens each have different responsibilities:\n",
    "\n",
    "-  [PAD] tokens are added to sentences so all of them have equal length\n",
    "-  [CLS] tokens are Classification tokens, it serves as a Start of sentence(SOS) and represent the meaning of the sentence\n",
    "- [SEP] represents the End of entence(EOS) and also separation between sentences\n",
    "- [MASK] is used for wordreplacement during masked language tasks\n",
    "- [UNK] is used for filling in for words that are out of the vocabulary og the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "tokenizer.train(\n",
    "    files=[\"nynorsk_corpus.txt\"],\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
    ")\n",
    "\n",
    "tokenizer.save_model(\"tokenizer/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just a small script for loading in the model from a directory\n",
    "Now we do not need to train it every time we want to test something quickly. Yippieee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Example tokens: ['dette', 'er', 'ein', 'test', 'for', 'nynorsk', '.']\n",
      "Vocabulary size: 30000\n",
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "# load_tokenizer.py\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the custom tokenizer from the saved directory.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "print(\"Tokenizer loaded. Example tokens:\", tokenizer.tokenize(\"Dette er ein test for Nynorsk.\"))\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCCTorchDataset\n",
    "Before sending our dataset into a BERT model we need to prepare it.\n",
    "In this class we use the tokenizer we trained earlier to encode the text included in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class NCCTorchDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer: BertTokenizer, max_len=128):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.hf_dataset[index]\n",
    "        text = item['text']\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
    "            'segment_ids': torch.zeros(encoded['input_ids'].shape, dtype=torch.long).squeeze(0)\n",
    "        }\n",
    "\n",
    "from datasets import load_dataset\n",
    "hf_dataset = load_dataset(\"NbAiLab/NCC\", split='train').select(range(5000))\n",
    "\n",
    "# Load the custom tokenizer (assumed to be trained and saved in the \"tokenizer/\" directory).\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "\n",
    "# Create our PyTorch dataset.\n",
    "dataset = NCCTorchDataset(hf_dataset, tokenizer, max_len=128)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the good stuff\n",
    "In the following cell we have prepared classes for positional embedding, BERT embedding and a CustomBERT model that we are going to train later. \n",
    "\n",
    "## Postitional embedding\n",
    "\n",
    "Positional embedding is the first part of the pipeline. It is a clever way of to destingush where in a sequence a word is placed, and also the distance to other words in the sequence. Sine and cosine functions are used to create unique positional encodings for each position in the sequence\n",
    "\n",
    "## BERT embedding\n",
    "The BERTembedding class is used to create input embeddings for the BERT model\n",
    "It combines token embeddings, segment embeddings and positional embeddings.\n",
    "The token embeddings are learned from the vocabularym while the segment embeddings are used tp diffriencate between two segments in the input. The positional embeddings are added to the token embeddings to provide information about the position of each token. Dropout layer is to prevent overfitting...\n",
    "\n",
    "# CustomBERT\n",
    "CustomBERT is a simplified version of the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # shape: [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.segment_embedding = nn.Embedding(2, d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(d_model, seq_len)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, token_ids, segment_ids):\n",
    "        x = self.token_embedding(token_ids) + self.segment_embedding(segment_ids)\n",
    "        x = self.positional_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# The CustomBERT class is a simplified version of the BERT model.\n",
    "# It consists of an embedding layer, a transformer encoder, and a masked language modeling (MLM) head.\n",
    "# The transformer encoder is composed of multiple layers, each containing multi-head self-attention\n",
    "# and feedforward neural networks.\n",
    "# The MLM head is used to predict the masked tokens in the input sequence.\n",
    "# The forward method takes token IDs, segment IDs, and an optional attention mask as input,\n",
    "# and returns the logits for the masked language modeling task.\n",
    "# The save_model method allows saving the model's state dictionary to a specified path.\n",
    "\n",
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, seq_len=128, num_layers=4, num_heads=4, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.embeddings = BERTEmbedding(vocab_size, d_model, seq_len)        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Masked Language Modeling head: project encoder output to the vocabulary space.\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, token_ids, segment_ids, attention_mask=None):\n",
    "        # token_ids, segment_ids: [batch_size, seq_len]\n",
    "        x = self.embeddings(token_ids, segment_ids)  # shape: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # PyTorch's Transformer encoder expects input shape: [seq_len, batch_size, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Create key padding mask if provided (mask positions where attention_mask==0)\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        # Pass through the encoder layers.\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        # Transform back to [batch_size, seq_len, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Compute MLM logits.\n",
    "        logits = self.mlm_head(x)  # shape: [batch_size, seq_len, vocab_size]\n",
    "        return logits\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Example tokens: ['dette', 'er', 'en', 'test', 'for', 'nynorsk', '.']\n",
      "Epoch 1/3 - Loss: 4.0497\n",
      "Epoch 2/3 - Loss: 1.4543\n",
      "Epoch 3/3 - Loss: 0.6466\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the custom tokenizer (assumed to be already trained and saved)\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "print(\"Tokenizer loaded. Example tokens:\", tokenizer.tokenize(\"Dette er en test for Nynorsk.\"))\n",
    "\n",
    "# Instantiate the custom BERT model.\n",
    "VOCAB_SIZE = 30000\n",
    "model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,\n",
    "                   num_layers=4, num_heads=4, hidden_dim=512)\n",
    "\n",
    "# Place the model on GPU if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop.\n",
    "num_epochs = 3\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        segment_ids = batch['segment_ids'].to(device)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(token_ids=input_ids, segment_ids=segment_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "        loss = loss_fct(logits.view(-1, VOCAB_SIZE), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss/len(dataloader):.4f}\")\n",
    "# Save the model after training.\n",
    "import os\n",
    "directory = \"custom_bert_model/\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "model.save_model(os.path.join(directory, \"custom_bert_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Example tokens: ['dette', 'er', 'en', 'test', 'for', 'nynorsk', '.']\n",
      "Test sentence: Dette er et godt eksempel [MASK] en setning.\n",
      "Predicted token: bak\n",
      "Predicted sentence: Dette er et godt eksempel bak en setning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "print(\"Tokenizer loaded. Example tokens:\", tokenizer.tokenize(\"Dette er en test for Nynorsk.\"))\n",
    "VOCAB_SIZE = 30000\n",
    "model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,\n",
    "                   num_layers=4, num_heads=4, hidden_dim=512)\n",
    "\n",
    "model.load_state_dict(torch.load(\"custom_bert_model/custom_bert_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a test sentence with a [MASK] token.\n",
    "test_sentence = \"Dette er et godt eksempel [MASK] en setning.\"\n",
    "print(\"Test sentence:\", test_sentence)\n",
    "\n",
    "# Tokenize input. Ensure the [MASK] token is preserved.\n",
    "encoded = tokenizer.encode_plus(\n",
    "    test_sentence,\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encoded[\"input_ids\"].to(device)\n",
    "attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "# For single-sentence input without segment differentiation, set segment_ids to zeros.\n",
    "segment_ids = torch.zeros_like(input_ids).to(device)\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    logits = model(token_ids=input_ids, segment_ids=segment_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Find the index of the [MASK] token in the input_ids.\n",
    "mask_token_index = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "if len(mask_token_index[0]) == 0:\n",
    "    raise ValueError(\"No [MASK] token found in the input!\")\n",
    "mask_index = mask_token_index[1].item()\n",
    "\n",
    "# Extract logits for the masked position and get the predicted token ID.\n",
    "predicted_token_logits = logits[0, mask_index]\n",
    "predicted_token_id = torch.argmax(predicted_token_logits).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
    "\n",
    "print(\"Predicted token:\", predicted_token)\n",
    "# Replace [MASK] with the predicted token in the string.\n",
    "predicted_sentence = test_sentence.replace(\"[MASK]\", predicted_token)\n",
    "print(\"Predicted sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector arithmetic result: mann - ungdom yields:\n",
      "1: mann (cosine similarity: 0.7066)\n",
      "2: byraet (cosine similarity: 0.2500)\n",
      "3: innvend (cosine similarity: 0.2408)\n",
      "4: ##ink (cosine similarity: 0.2339)\n",
      "5: ##produksjon (cosine similarity: 0.2328)\n",
      "6: ##ese (cosine similarity: 0.2317)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Assume CustomBERT is defined in your code (see previous examples)\n",
    "# from custom_bert import CustomBERT\n",
    "\n",
    "# Load the custom tokenizer and model.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "model = CustomBERT(vocab_size=30000, d_model=256, seq_len=128, num_layers=4, num_heads=4, hidden_dim=512)\n",
    "model.load_state_dict(torch.load(\"custom_bert_model/custom_bert_model.pth\"))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_token_vector(word, tokenizer, model):\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    if not tokens:\n",
    "        raise ValueError(f\"Word '{word}' could not be tokenized.\")\n",
    "    token_id = tokenizer.convert_tokens_to_ids(tokens[0])\n",
    "    return model.embeddings.token_embedding.weight[token_id]\n",
    "\n",
    "word_a = \"mann\"\n",
    "word_b = \"ungdom\"\n",
    "\n",
    "vec_a = get_token_vector(word_a, tokenizer, model)\n",
    "vec_b = get_token_vector(word_b, tokenizer, model)\n",
    "\n",
    "target_vector = vec_a - vec_b\n",
    "\n",
    "# Compute cosine similarities with all token embeddings.\n",
    "all_embeddings = model.embeddings.token_embedding.weight\n",
    "target_vector_norm = F.normalize(target_vector.unsqueeze(0), dim=-1)\n",
    "all_embeddings_norm = F.normalize(all_embeddings, dim=-1)\n",
    "cosine_sim = torch.matmul(target_vector_norm, all_embeddings_norm.transpose(0, 1))\n",
    "\n",
    "# Retrieve the top 3 tokens with highest cosine similarity.\n",
    "topk = torch.topk(cosine_sim, k=6)\n",
    "top_values = topk.values.squeeze(0).tolist()\n",
    "top_indices = topk.indices.squeeze(0).tolist()\n",
    "\n",
    "print(f\"Vector arithmetic result: {word_a} - {word_b} yields:\")\n",
    "for i, (score, idx) in enumerate(zip(top_values, top_indices)):\n",
    "    token = tokenizer.convert_ids_to_tokens(idx)\n",
    "    print(f\"{i+1}: {token} (cosine similarity: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
