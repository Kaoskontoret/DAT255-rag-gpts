{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frist we need to load our dataset\n",
    "We have selected the NbAiLab/NCC dataset\n",
    "This is a dataset contructed by the national library optimized for training large language models\n",
    "We only want the nynorsk eksamples since our bert model are focusing on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "\n",
    "# Load the dataset in streaming mode.\n",
    "dataset = load_dataset(\"NbAiLab/NCC\", streaming=True)\n",
    "\n",
    "# Use the 'train' split from the dataset.\n",
    "# Note: Make sure to filter on the correct language field.\n",
    "nynorsk_dataset = dataset[\"train\"].filter(lambda example: example.get(\"lang_fasttext\", \"\") == \"nn\")\n",
    "\n",
    "# Write a subset (e.g., first 1000 examples) to a file using islice.\n",
    "with open(\"nynorsk_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, example in enumerate(islice(nynorsk_dataset, 2000)):\n",
    "        f.write(example[\"text\"].strip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and settin special tokens\n",
    "Before we start creating hour BERT model we first define our tokenizer. The tokenizer builds a tokenizermodel working with a set vocabulary size.\n",
    "\n",
    "It is also to define some special tokens. These tokens each have different responsibilities:\n",
    "\n",
    "-  [PAD] tokens are added to sentences so all of them have equal length\n",
    "-  [CLS] tokens are Classification tokens, it serves as a Start of sentence(SOS) and represent the meaning of the sentence\n",
    "- [SEP] represents the End of entence(EOS) and also separation between sentences\n",
    "- [MASK] is used for wordreplacement during masked language tasks\n",
    "- [UNK] is used for filling in for words that are out of the vocabulary og the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "tokenizer.train(\n",
    "    files=[\"nynorsk_corpus.txt\"],\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
    ")\n",
    "\n",
    "tokenizer.save_model(\"tokenizer/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just a small script for loading in the tokenizer model from a directory\n",
    "Now we do not need to train it every time we want to test something quickly. Yippieee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Example tokens: ['dette', 'er', 'ein', 'test', 'for', 'nynorsk', '.']\n",
      "Vocabulary size: 30000\n",
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "# load_tokenizer.py\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the custom tokenizer from the saved directory.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "print(\"Tokenizer loaded. Example tokens:\", tokenizer.tokenize(\"Dette er ein test for Nynorsk.\"))\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCCTorchDataset\n",
    "Before sending our dataset into a BERT model we need to prepare it.\n",
    "In this class we use the tokenizer we trained earlier to encode the text included in the dataset.\n",
    "\n",
    "Here an important feature to mention is the DataCollectorForLanguageModdeling class. This masks a procentage of the dataset so it will be prime to use for training a BERT model for mlm tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2000 examples [00:00, 55853.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([32, 128]), 'attention_mask': torch.Size([32, 128]), 'token_type_ids': torch.Size([32, 128]), 'labels': torch.Size([32, 128])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "class NCCTorchDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer: BertTokenizer, max_len=128):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.max_len    = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.hf_dataset[index]['text']\n",
    "        enc  = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids      = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        # rename segment_ids → token_type_ids\n",
    "        token_type_ids = torch.zeros_like(input_ids, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "\n",
    "# 1) load HF text dataset\n",
    "from datasets import load_dataset\n",
    "hf_dataset = load_dataset(\"text\", data_files={\"train\": \"nynorsk_corpus.txt\"}, split=\"train\")\n",
    "\n",
    "# 2) load your tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "\n",
    "# 3) wrap in our Dataset\n",
    "dataset = NCCTorchDataset(hf_dataset, tokenizer, max_len=128)\n",
    "\n",
    "# 4) create an MLM collator (15% masked tokens)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# 5) DataLoader with dynamic masking\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Sanity check one batch\n",
    "batch = next(iter(dataloader))\n",
    "print({k: v.shape for k, v in batch.items()})\n",
    "# e.g.: {\n",
    "#   'input_ids':      torch.Size([32, 128]),\n",
    "#   'attention_mask': torch.Size([32, 128]),\n",
    "#   'token_type_ids': torch.Size([32, 128]),\n",
    "#   'labels':         torch.Size([32, 128])\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the good stuff\n",
    "In the following cell we have prepared classes for positional embedding, BERT embedding and a CustomBERT model that we are going to train later. \n",
    "\n",
    "## Postitional embedding\n",
    "\n",
    "Positional embedding is the first part of the pipeline. It is a clever way of to destingush where in a sequence a word is placed, and also the distance to other words in the sequence. Sine and cosine functions are used to create unique positional encodings for each position in the sequence\n",
    "\n",
    "## BERT embedding\n",
    "The BERTembedding class is used to create input embeddings for the BERT model\n",
    "It combines token embeddings, segment embeddings and positional embeddings.\n",
    "The token embeddings are learned from the vocabularym while the segment embeddings are used tp diffriencate between two segments in the input. The positional embeddings are added to the token embeddings to provide information about the position of each token. Dropout layer is to prevent overfitting...\n",
    "\n",
    "# CustomBERT\n",
    "CustomBERT is a simplified version of the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.segment_embedding = nn.Embedding(2, d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(d_model, seq_len)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, token_ids, segment_ids):\n",
    "        x = self.token_embedding(token_ids) + self.segment_embedding(segment_ids)\n",
    "        x = self.positional_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# The CustomBERT class is a simplified version of the BERT model.\n",
    "# It consists of an embedding layer, a transformer encoder, and a masked language modeling (MLM) head.\n",
    "# The transformer encoder is composed of multiple layers, each containing multi-head self-attention\n",
    "# and feedforward neural networks.\n",
    "# The MLM head is used to predict the masked tokens in the input sequence.\n",
    "# The forward method takes token IDs, segment IDs, and an optional attention mask as input,\n",
    "# and returns the logits for the masked language modeling task.\n",
    "# The save_model method allows saving the model's state dictionary to a specified path.\n",
    "\n",
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, seq_len=128, num_layers=4, num_heads=4, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.embeddings = BERTEmbedding(vocab_size, d_model, seq_len)        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Masked Language Modeling head: project encoder output to the vocabulary space.\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, token_ids, segment_ids, attention_mask=None):\n",
    "        # token_ids, segment_ids: [batch_size, seq_len]\n",
    "        x = self.embeddings(token_ids, segment_ids)  # shape: [batch_size, seq_len, d_model]\n",
    "        # PyTorch's Transformer encoder expects input shape: [seq_len, batch_size, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        # Create key padding mask if provided (mask positions where attention_mask==0)\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None \n",
    "        # Pass through the encoder layers.\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        # Transform back to [batch_size, seq_len, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        # Compute MLM logits.\n",
    "        logits = self.mlm_head(x)  # shape: [batch_size, seq_len, vocab_size]\n",
    "        return logits\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we train our own customBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following steps are used\n",
    "\n",
    "Create a model and what hyperparameters it should have.\n",
    "\n",
    "Optimizer\n",
    "We use an adamW optimizer for updating the models parameters based on gradients computed during back propagation\n",
    "\n",
    "Loss function\n",
    "The loss function is a crossentropu loss that computes the difference between our models predicted token distribution, and the \"true\" tokens it want it to predikt\n",
    "by setting all unmasked positions in the labels to -100, we kan ignore index -100 to skip those positions when averaging the loss. This penalizes the model for getting masked tokens wrong, matching BERT mlm objective\n",
    "\n",
    "GradScaler\n",
    "Then we use a gradient scaler. This utilizes 16.bit operations which uses a lott less memory than the 32 bit operations. CHATGPT said this will help so we tried it\n",
    "\n",
    "## Then the epochs\n",
    "\n",
    "each epoch loops trhough the whole dataset once. The first lines load the tensors from the batch. Then zero_grad() is added to clear gradient buffers from previous steps.\n",
    "Autocast enables mixed-precision, and reshapes the tensors from [B*L,V] whre B are batch size, L sequence lenght and V the Vocab_length, down to [B*L] this makes it possible for CrossEntropyLoss to compute the average negative log-likelygood over the masked tokens\n",
    "\n",
    "Backpropagation is done with the gradscaler to correct the model taking the los as an input\n",
    "\n",
    "These teqniques were not used earlier for some models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  24%|██▍       | 15/63 [39:18<2:04:41, 155.86s/batch, avg_loss=10.0459]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=4, num_heads=4, hidden_dim=512)\n",
    "\n",
    "\n",
    "# 1) Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# 2) Loss only computed on masked tokens\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# 3) mixed precision\n",
    "scaler = torch.GradScaler(\"cpu\")\n",
    "\n",
    "model.train()\n",
    "num_epochs = 3\n",
    "\n",
    "print(\"Training started...\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # wrap the loader and tell tqdm how many batches to expect\n",
    "    loop = tqdm(dataloader, total=len(dataloader), desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
    "\n",
    "    for step, batch in enumerate(loop, start=1):\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(\"cpu\"):\n",
    "            logits = model(\n",
    "                token_ids=input_ids,\n",
    "                segment_ids=token_type_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, VOCAB_SIZE),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / step\n",
    "\n",
    "        # now safe to divide by `step`, which is >= 1\n",
    "        loop.set_postfix(avg_loss=f\"{avg_loss:.4f}\")\n",
    "\n",
    "    print(f\"*** Epoch {epoch} complete — avg loss: {total_loss/len(dataloader):.4f} ***\")\n",
    "\n",
    "\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"custom_bert_model/mlm_finetuned.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 predictions for [MASK]: ['.', \"'\", 'i', ',', 'og', 'er', 'pa', 'til', 'av', 'det']\n",
      "Original sentence: Dette [MASK] ein test for Nynorsk.\n",
      "Masked token ID: 18\n",
      "Masked token: [MASK]\n",
      "Prediction for [MASK]: .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer  # pip install transformers\n",
    "\n",
    "# 1) Load tokenizer & model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "\n",
    "# Load the custom BERT model with tuned parameters trained on 2000 examples.\n",
    "#model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=12, num_heads=12, hidden_dim=512)\n",
    "\n",
    "\n",
    "# Load model tuned with parameters trained on 8000 examples.\n",
    "model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=12, num_heads=12, hidden_dim=512)\n",
    "\n",
    "# Load model with smaller parameters trainied on 10000 examples.\n",
    "# model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,num_layers=4, num_heads=4, hidden_dim=512)\n",
    "\n",
    "\n",
    "# Load model trained on 100000 examples using the same parameters as the small, with a gpu.\n",
    "#model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,num_layers=4, num_heads=4, hidden_dim=512)\n",
    "\n",
    "# Load model from directory.\n",
    "current_model = \"custom_bert_model3.pth\"\n",
    "checkpoint = torch.load(\n",
    "    \"custom_bert_model/\" + current_model,\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# 2) Prepare a masked sentence\n",
    "sentence = \"Dette [MASK] ein test for Nynorsk.\"\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "token_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "segment_ids = torch.zeros_like(token_ids)\n",
    "\n",
    "# 3) Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(token_ids, segment_ids, attention_mask)\n",
    "\n",
    "# 4) Find masked position(s) and predict\n",
    "mask_token_index = (token_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "batch_idx, seq_pos = mask_token_index[0][0], mask_token_index[1][0]\n",
    "mask_logits = logits[batch_idx, seq_pos]\n",
    "# Get the 5 most likely tokens for the masked position.\n",
    "top_k = 10\n",
    "top_k_indices = torch.topk(mask_logits, top_k).indices\n",
    "top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices.tolist())\n",
    "# Print the top-k predictions\n",
    "print(f\"Top {top_k} predictions for [MASK]: {top_k_tokens}\")\n",
    "# 5) Get the predicted token ID and convert it to a token\n",
    "predicted_id = mask_logits.argmax(dim=-1).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_id)\n",
    "# 5) Print the prediction\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Masked token ID: {predicted_id}\")\n",
    "print(f\"Masked token: {tokenizer.mask_token}\")\n",
    "print(f\"Prediction for [MASK]: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try to score the models on classification\n",
    "We will wrap the CustomBERT in a BertForSequenceClassification\n",
    "\n",
    "Then we gather all preds and all labels as numpy arrays\n",
    "\n",
    "for metrics we create a classifivation_report that shows precision/recall/F! scores\n",
    "\n",
    "end we will visualize using a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, bert: CustomBERT, hidden_dim=768, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, num_labels)\n",
    "        )\n",
    "    def forward(self, input_ids, segment_ids, attention_mask):\n",
    "        # get the pooled representation (here we just take the [CLS] token at pos 0)\n",
    "        last_hidden = self.bert(input_ids, segment_ids, attention_mask)\n",
    "        cls_repr   = last_hidden[:, 0, :]           # shape [B, hidden_dim]\n",
    "        logits     = self.classifier(cls_repr)      # [B, num_labels]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jobjornrokenesmyren/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): CustomBERT(\n",
       "    (embeddings): BERTEmbedding(\n",
       "      (token_embedding): Embedding(30000, 768)\n",
       "      (segment_embedding): Embedding(2, 768)\n",
       "      (positional_embedding): PositionalEmbedding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mlm_head): Linear(in_features=768, out_features=30000, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "# load your checkpoint & map to CPU/GPU:\n",
    "bert = CustomBERT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=768, seq_len=128,\n",
    "    num_layers=12, num_heads=12, hidden_dim=512\n",
    ")\n",
    "checkpoint = torch.load(\"custom_bert_model/custom_bert_model2.pth\",\n",
    "                        map_location=\"cpu\")\n",
    "bert.load_state_dict(checkpoint)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert.to(device).eval()\n",
    "\n",
    "# Wrap in classification model\n",
    "model = BertForSequenceClassification(bert, hidden_dim=768, num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "def filter_noisy(example):\n",
    "    lang_ok = example[\"language\"] in [\"nb\", \"nn\"]\n",
    "    conf_ok = example[\"language_confidence\"] >= 0.8\n",
    "    return lang_ok and conf_ok\n",
    "\n",
    "ncc_filtered = ncc.filter(filter_noisy, num_proc=4)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids      = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels         = batch['labels'].to(device)\n",
    "        segment_ids    = torch.zeros_like(input_ids)  # single‑sentence\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, segment_ids, attention_mask)\n",
    "        loss   = loss_fn(\n",
    "            logits.view(-1, tokenizer.vocab_size),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x30000 and 768x768)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m      9\u001b[39m segment_ids    = batch[\u001b[33m\"\u001b[39m\u001b[33msegment_ids\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m preds  = torch.argmax(logits, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     13\u001b[39m all_preds.append(preds.cpu().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, segment_ids, attention_mask)\u001b[39m\n\u001b[32m     13\u001b[39m last_hidden = \u001b[38;5;28mself\u001b[39m.bert(input_ids, segment_ids, attention_mask)\n\u001b[32m     14\u001b[39m cls_repr   = last_hidden[:, \u001b[32m0\u001b[39m, :]           \u001b[38;5;66;03m# shape [B, hidden_dim]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m logits     = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_repr\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# [B, num_labels]\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (2x30000 and 768x768)"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        segment_ids    = batch[\"segment_ids\"].to(device)\n",
    "        labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, segment_ids, attention_mask)\n",
    "        preds  = torch.argmax(logits, dim=-1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_preds  = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(\"Classification report:\\n\")\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"]))\n",
    "\n",
    "macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "lables_unique = sorted(set(all_labels.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(\n",
    "    xticks=np.arange(len(labels_unique)),\n",
    "    yticks=np.arange(len(labels_unique)),\n",
    "    xticklabels=labels_unique,\n",
    "    yticklabels=labels_unique,\n",
    "    ylabel='True label',\n",
    "    xlabel='Predicted label',\n",
    "    title='Confusion Matrix'\n",
    ")\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets train a predefined bert model from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch import optim\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "import random\n",
    "\n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling.\n",
    "    This function randomly masks tokens with a probability of mlm_probability.\n",
    "    \"\"\"\n",
    "    labels = inputs.clone()\n",
    "    # Create a mask of positions to mask\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "        for val in labels.tolist()\n",
    "    ]\n",
    "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # Only compute loss on masked tokens\n",
    "\n",
    "    # Replace masked indices with the [MASK] token id\n",
    "    inputs[masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    return inputs, labels\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # Move inputs to device:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['segment_ids'].to(device)\n",
    "\n",
    "        # Apply masking for MLM:\n",
    "        masked_input_ids, labels = mask_tokens(input_ids.clone(), tokenizer, mlm_probability=0.15)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=masked_input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels.to(device)\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomBERT' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m os.makedirs(directory, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mfinetuned_bert_model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m(os.path.join(directory, \u001b[33m\"\u001b[39m\u001b[33mBERTmlm.pth\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'CustomBERT' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "# Save the model after training.\n",
    "directory = \"custom_bert_model/\"\n",
    "import os\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "model_name = \"finetuned_bert_model\"\n",
    "model.save_pretrained(os.path.join(directory, \"BERTmlm.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while deserializing header: MetadataIncompleteBuffer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model_dir = \u001b[33m\"\u001b[39m\u001b[33mcustom_bert_model/BERTmlm\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m tokenizer_dir = \u001b[33m\"\u001b[39m\u001b[33mtokenizer/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mBertForMaskedLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m tokenizer = BertTokenizer.from_pretrained(tokenizer_dir)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Create the fill-mask pipeline\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:272\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    274\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bachelor/Prosjekt-255/DAT255-rag-gpts/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4345\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4337\u001b[39m is_from_file = pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4340\u001b[39m     is_safetensors_available()\n\u001b[32m   4341\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_from_file\n\u001b[32m   4342\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded\n\u001b[32m   4343\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m checkpoint_files[\u001b[32m0\u001b[39m].endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4344\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m4345\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   4346\u001b[39m         metadata = f.metadata()\n\u001b[32m   4348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4349\u001b[39m         \u001b[38;5;66;03m# Assume it's a pytorch checkpoint (introduced for timm checkpoints)\u001b[39;00m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while deserializing header: MetadataIncompleteBuffer"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load your fine-tuned model and your custom tokenizer\n",
    "model_dir = \"custom_bert_model/BERTmlm\"\n",
    "tokenizer_dir = \"tokenizer/\"\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "# Create the fill-mask pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a test sentence with a [MASK] token.\n",
    "# (Make sure the special mask token in your tokenizer matches what you use here.)\n",
    "test_sentence = \"Dette er et eksempel på [MASK] norsk setning.\"\n",
    "results = fill_mask(test_sentence)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Prediction: {result['token_str']}, Score: {result['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector arithmetic result: dag + seint yields:\n",
      "1: seint (cosine similarity: 0.7218)\n",
      "2: dag (cosine similarity: 0.6949)\n",
      "3: ##nende (cosine similarity: 0.2359)\n",
      "4: admin (cosine similarity: 0.2355)\n",
      "5: ferdene (cosine similarity: 0.2219)\n",
      "6: ##snemnda (cosine similarity: 0.2189)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Assume CustomBERT is defined in your code (see previous examples)\n",
    "# from custom_bert import CustomBERT\n",
    "\n",
    "# Load the custom tokenizer and model.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "# model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=768, seq_len=128,num_layers=12, num_heads=12, hidden_dim=512)\n",
    "model = CustomBERT(vocab_size=30000, d_model=256, seq_len=128, num_layers=4, num_heads=4, hidden_dim=512)\n",
    "model.load_state_dict(torch.load(\"custom_bert_model/i1_custom_bert_model.pth\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_token_vector(word, tokenizer, model):\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    if not tokens:\n",
    "        raise ValueError(f\"Word '{word}' could not be tokenized.\")\n",
    "    token_id = tokenizer.convert_tokens_to_ids(tokens[0])\n",
    "    return model.embeddings.token_embedding.weight[token_id]\n",
    "\n",
    "word_a = \"dag\"\n",
    "word_b = \"seint\"\n",
    "\n",
    "vec_a = get_token_vector(word_a, tokenizer, model)\n",
    "vec_b = get_token_vector(word_b, tokenizer, model)\n",
    "\n",
    "target_vector = vec_a + vec_b\n",
    "\n",
    "# Compute cosine similarities with all token embeddings.\n",
    "all_embeddings = model.embeddings.token_embedding.weight\n",
    "target_vector_norm = F.normalize(target_vector.unsqueeze(0), dim=-1)\n",
    "all_embeddings_norm = F.normalize(all_embeddings, dim=-1)\n",
    "cosine_sim = torch.matmul(target_vector_norm, all_embeddings_norm.transpose(0, 1))\n",
    "\n",
    "# Retrieve the top 3 tokens with highest cosine similarity.\n",
    "topk = torch.topk(cosine_sim, k=6)\n",
    "top_values = topk.values.squeeze(0).tolist()\n",
    "top_indices = topk.indices.squeeze(0).tolist()\n",
    "\n",
    "print(f\"Vector arithmetic result: {word_a} + {word_b} yields:\")\n",
    "for i, (score, idx) in enumerate(zip(top_values, top_indices)):\n",
    "    token = tokenizer.convert_ids_to_tokens(idx)\n",
    "    print(f\"{i+1}: {token} (cosine similarity: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
