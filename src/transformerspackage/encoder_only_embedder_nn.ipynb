{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "\n",
    "# Load the dataset in streaming mode.\n",
    "dataset = load_dataset(\"NbAiLab/NCC\", streaming=True)\n",
    "\n",
    "# Use the 'train' split from the dataset.\n",
    "# Note: Make sure to filter on the correct language field.\n",
    "nynorsk_dataset = dataset[\"train\"].filter(lambda example: example.get(\"lang_fasttext\", \"\") == \"nn\")\n",
    "\n",
    "# Write a subset (e.g., first 1000 examples) to a file using islice.\n",
    "with open(\"nynorsk_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, example in enumerate(islice(nynorsk_dataset, 10000)):\n",
    "        f.write(example[\"text\"].strip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "tokenizer.train(\n",
    "    files=[\"nynorsk_corpus.txt\"],\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
    ")\n",
    "tokenizer.save_model(\"tokenizer/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Example tokens: ['dette', 'er', 'ein', 'test', 'for', 'nynorsk', '.']\n",
      "Vocabulary size: 30000\n",
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "# load_tokenizer.py\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the custom tokenizer from the saved directory.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "print(\"Tokenizer loaded. Example tokens:\", tokenizer.tokenize(\"Dette er ein test for Nynorsk.\"))\n",
    "# Check the vocabulary size\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "# Check the special tokens\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class NCCTorchDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer: BertTokenizer, max_len=128):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Assume each item has a 'text' field.\n",
    "        item = self.hf_dataset[index]\n",
    "        text = item['text']\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),         # shape: [max_len]\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(0),   # shape: [max_len]\n",
    "            # If no segment info is available, default to zeros.\n",
    "            'segment_ids': torch.zeros(encoded['input_ids'].shape, dtype=torch.long).squeeze(0)\n",
    "        }\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. Load the NCC Dataset from Hugging Face\n",
    "# -----------------------------------------\n",
    "# Load the \"NbAiLab/NCC\" dataset using the Hugging Face datasets library.\n",
    "# For demonstration, we select a small subset (e.g., first 100 samples).\n",
    "hf_dataset = load_dataset(\"NbAiLab/NCC\", split='train').select(range(5000))\n",
    "\n",
    "# Load the custom tokenizer (assumed to be trained and saved in the \"tokenizer/\" directory).\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create our PyTorch dataset.\n",
    "dataset = NCCTorchDataset(hf_dataset, tokenizer, max_len=128)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# =======================\n",
    "# 1. Custom Embedding Modules\n",
    "# =======================\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # shape: [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, d_model]\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, seq_len=128):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # Here we assume 2 segments (e.g. sentence A and sentence B)\n",
    "        self.segment_embedding = nn.Embedding(2, d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(d_model, seq_len)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, token_ids, segment_ids):\n",
    "        # token_ids and segment_ids: [batch_size, seq_len]\n",
    "        x = self.token_embedding(token_ids) + self.segment_embedding(segment_ids)\n",
    "        x = self.positional_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "# =======================\n",
    "# 2. Custom BERT Model Definition\n",
    "# =======================\n",
    "\n",
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, seq_len=128, num_layers=4, num_heads=4, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.embeddings = BERTEmbedding(vocab_size, d_model, seq_len)\n",
    "        \n",
    "        # Define a Transformer encoder (stack of encoder layers)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Masked Language Modeling head: project encoder output to the vocabulary space.\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, token_ids, segment_ids, attention_mask=None):\n",
    "        # token_ids, segment_ids: [batch_size, seq_len]\n",
    "        x = self.embeddings(token_ids, segment_ids)  # shape: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # PyTorch's Transformer encoder expects input shape: [seq_len, batch_size, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Create key padding mask if provided (mask positions where attention_mask==0)\n",
    "        key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "        \n",
    "        # Pass through the encoder layers.\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        # Transform back to [batch_size, seq_len, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Compute MLM logits.\n",
    "        logits = self.mlm_head(x)  # shape: [batch_size, seq_len, vocab_size]\n",
    "        return logits\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Example tokens: ['dette', 'er', 'en', 'test', 'for', 'nynorsk', '.']\n",
      "Epoch 1/3 - Loss: 4.0497\n",
      "Epoch 2/3 - Loss: 1.4543\n",
      "Epoch 3/3 - Loss: 0.6466\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the custom tokenizer (assumed to be already trained and saved)\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "print(\"Tokenizer loaded. Example tokens:\", tokenizer.tokenize(\"Dette er en test for Nynorsk.\"))\n",
    "\n",
    "# Instantiate the custom BERT model.\n",
    "VOCAB_SIZE = 30000  # Should match the vocabulary size of your tokenizer.\n",
    "model = CustomBERT(vocab_size=VOCAB_SIZE, d_model=256, seq_len=128,\n",
    "                   num_layers=4, num_heads=4, hidden_dim=512)\n",
    "\n",
    "# Place the model on GPU if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop.\n",
    "num_epochs = 3\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        segment_ids = batch['segment_ids'].to(device)\n",
    "\n",
    "        # In a full MLM training scenario, you would apply a masking strategy.\n",
    "        # For this simplified example, we'll use input_ids as labels directly.\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(token_ids=input_ids, segment_ids=segment_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Compute loss: flatten logits and labels.\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "        loss = loss_fct(logits.view(-1, VOCAB_SIZE), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss/len(dataloader):.4f}\")\n",
    "# Save the model after training.\n",
    "import os\n",
    "directory = \"custom_bert_model/\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "model.save_model(os.path.join(directory, \"custom_bert_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. Example tokens: ['dette', 'er', 'en', 'test', 'for', 'nynorsk', '.']\n",
      "Test sentence: Dette er et godt eksempel [MASK] en setning.\n",
      "Predicted token: lin\n",
      "Predicted sentence: Dette er et godt eksempel lin en setning.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "print(\"Tokenizer loaded. Example tokens:\", tokenizer.tokenize(\"Dette er en test for Nynorsk.\"))\n",
    "model.load_state_dict(torch.load(\"custom_bert_model/custom_bert_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a test sentence with a [MASK] token.\n",
    "test_sentence = \"Dette er et godt eksempel [MASK] en setning.\"\n",
    "print(\"Test sentence:\", test_sentence)\n",
    "\n",
    "# Tokenize input. Ensure the [MASK] token is preserved.\n",
    "encoded = tokenizer.encode_plus(\n",
    "    test_sentence,\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encoded[\"input_ids\"].to(device)\n",
    "attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "# For single-sentence input without segment differentiation, set segment_ids to zeros.\n",
    "segment_ids = torch.zeros_like(input_ids).to(device)\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    logits = model(token_ids=input_ids, segment_ids=segment_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Find the index of the [MASK] token in the input_ids.\n",
    "mask_token_index = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "if len(mask_token_index[0]) == 0:\n",
    "    raise ValueError(\"No [MASK] token found in the input!\")\n",
    "# Here, we assume only one [MASK] token per input.\n",
    "mask_index = mask_token_index[1].item()\n",
    "\n",
    "# Extract logits for the masked position and get the predicted token ID.\n",
    "predicted_token_logits = logits[0, mask_index]\n",
    "predicted_token_id = torch.argmax(predicted_token_logits).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
    "\n",
    "print(\"Predicted token:\", predicted_token)\n",
    "# Replace [MASK] with the predicted token in the string.\n",
    "predicted_sentence = test_sentence.replace(\"[MASK]\", predicted_token)\n",
    "print(\"Predicted sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector arithmetic result: konge - mann + kvinne yields:\n",
      "1: kvinne (cosine similarity: 0.6046)\n",
      "2: konge (cosine similarity: 0.5614)\n",
      "3: livsl (cosine similarity: 0.2522)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Assume CustomBERT is defined in your code (see previous examples)\n",
    "# from custom_bert import CustomBERT\n",
    "\n",
    "# Load the custom tokenizer and model.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tokenizer/\")\n",
    "model = CustomBERT(vocab_size=30000, d_model=256, seq_len=128, num_layers=4, num_heads=4, hidden_dim=512)\n",
    "model.load_state_dict(torch.load(\"custom_bert_model/custom_bert_model.pth\"))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Helper function to get the embedding vector for a given word.\n",
    "def get_token_vector(word, tokenizer, model):\n",
    "    # Tokenize the word.\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    if not tokens:\n",
    "        raise ValueError(f\"Word '{word}' could not be tokenized.\")\n",
    "    # For simplicity, assume the word corresponds to a single token.\n",
    "    token_id = tokenizer.convert_tokens_to_ids(tokens[0])\n",
    "    # Return the token embedding.\n",
    "    return model.embeddings.token_embedding.weight[token_id]\n",
    "\n",
    "word_a = \"konge\"\n",
    "word_b = \"mann\"\n",
    "word_c = \"kvinne\" \n",
    "\n",
    "vec_a = get_token_vector(word_a, tokenizer, model)\n",
    "vec_b = get_token_vector(word_b, tokenizer, model)\n",
    "vec_c = get_token_vector(word_c, tokenizer, model)\n",
    "\n",
    "target_vector = vec_a - vec_b + vec_c\n",
    "\n",
    "# Compute cosine similarities with all token embeddings.\n",
    "all_embeddings = model.embeddings.token_embedding.weight\n",
    "target_vector_norm = F.normalize(target_vector.unsqueeze(0), dim=-1)\n",
    "all_embeddings_norm = F.normalize(all_embeddings, dim=-1)\n",
    "cosine_sim = torch.matmul(target_vector_norm, all_embeddings_norm.transpose(0, 1))\n",
    "\n",
    "# Retrieve the top 3 tokens with highest cosine similarity.\n",
    "topk = torch.topk(cosine_sim, k=3)\n",
    "top_values = topk.values.squeeze(0).tolist()\n",
    "top_indices = topk.indices.squeeze(0).tolist()\n",
    "\n",
    "print(f\"Vector arithmetic result: {word_a} - {word_b} + {word_c} yields:\")\n",
    "for i, (score, idx) in enumerate(zip(top_values, top_indices)):\n",
    "    token = tokenizer.convert_ids_to_tokens(idx)\n",
    "    print(f\"{i+1}: {token} (cosine similarity: {score:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
